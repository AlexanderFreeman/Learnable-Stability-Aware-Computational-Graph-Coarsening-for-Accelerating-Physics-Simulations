{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoXQjbRzihjA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torch-geometric==2.5.0\n",
        "!pip install trimesh\n",
        "!pip install differentiable-voronoi\n",
        "!pip install gradnorm-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htJX4B_Ai7HL"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import pydmd as dmd\n",
        "except ImportError:\n",
        "  !pip install pydmd==<2025.5.1>\n",
        "  import pydmd as dmd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N44a38Toi_sR"
      },
      "outputs": [],
      "source": [
        "from differentiable_voronoi import differentiable_voronoi, triangulate\n",
        "from shapely import Polygon, LineString, Point\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import to_dense_adj, to_undirected, degree, to_dense_batch\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans, SpectralClustering, BisectingKMeans\n",
        "from trimesh import Trimesh\n",
        "from scipy.spatial import Delaunay\n",
        "import matplotlib.patches\n",
        "from shapely.geometry.polygon import orient\n",
        "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
        "from torch_geometric.nn.pool import avg_pool_x\n",
        "import copy\n",
        "from torch.nn import MSELoss\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neighbors import KDTree\n",
        "from time import sleep\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from torch_geometric.nn import GCNConv, DenseGraphConv, GraphConv, GATConv\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch_geometric.nn.dense.mincut_pool import _rank3_trace\n",
        "from torch.nn.functional import cosine_similarity\n",
        "import torch.nn.utils as nn_utils\n",
        "from torch_geometric.nn.models.mlp import MLP\n",
        "from gradnorm_pytorch import GradNormLossWeighter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmVa6NbnjGNr"
      },
      "outputs": [],
      "source": [
        "# these functions are needed for slightly modified differentiable_voronoi function needed for calculating boundary nodes\n",
        "\n",
        "from differentiable_voronoi.differentiable_voronoi import center_of_circumcircle, complete_boundary_cells, orient_vertex_faces_dict, crop_cells, select_edges_adjacent_to_the_given_vetrices, calc_lengths_of_voronoi_edges_adjacent_to_vertices_to_crop, calc_lengths_of_inner_voronoi_edges, calc_areas_dict_bounded, draw_voronoi_region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsRlJ2SDjQzS"
      },
      "outputs": [],
      "source": [
        "# modified differentiable voronoi function to obtain boundary nodes\n",
        "\n",
        "def differentiable_voronoi(coords, mesh, boundary=None, vizualize=False):\n",
        "    vertex_faces_dict = {}\n",
        "    for i, row in enumerate(mesh.vertex_faces):\n",
        "        vertex_faces_dict[i] = [face for face in row if face != -1]\n",
        "\n",
        "    assert len(vertex_faces_dict) == len(coords), print(len(vertex_faces_dict), len(coords))\n",
        "\n",
        "    circumcenters = center_of_circumcircle(coords, torch.tensor(mesh.faces))\n",
        "    clipped_vertices_dict = None\n",
        "\n",
        "    circumcenters, vertex_faces_dict = complete_boundary_cells(coords, vertex_faces_dict, circumcenters,\n",
        "                                                               mesh.boundary_nodes, mesh)\n",
        "    assert len(vertex_faces_dict) == len(coords), print(len(vertex_faces_dict), len(coords))\n",
        "    circumcenters, vertex_faces_dict = orient_vertex_faces_dict(coords, vertex_faces_dict, circumcenters,\n",
        "                                                                mesh.boundary_nodes)\n",
        "    assert len(vertex_faces_dict) == len(coords)\n",
        "\n",
        "    if boundary is not None:\n",
        "        clipped_vertices_dict, voronoi_region_to_keep, voronoi_regions_to_crop = crop_cells(coords, vertex_faces_dict,\n",
        "                                                                                            circumcenters, boundary)\n",
        "        assert len(clipped_vertices_dict) == len(coords)\n",
        "        edges_adjacent_to_vertices_to_crop = select_edges_adjacent_to_the_given_vetrices(voronoi_regions_to_crop,\n",
        "                                                                                         mesh.edges_unique)\n",
        "        lengths_of_voronoi_edges, edges_to_delete = calc_lengths_of_voronoi_edges_adjacent_to_vertices_to_crop(\n",
        "            edges_adjacent_to_vertices_to_crop,\n",
        "            clipped_vertices_dict,\n",
        "            mesh.edges_unique)\n",
        "\n",
        "        lengths_of_voronoi_edges = calc_lengths_of_inner_voronoi_edges(\n",
        "            set(list(range(len(mesh.edges_unique)))) - edges_adjacent_to_vertices_to_crop,\n",
        "            circumcenters,\n",
        "            lengths_of_voronoi_edges,\n",
        "            mesh.edges_unique_faces)\n",
        "        lengths_of_voronoi_edges = lengths_of_voronoi_edges[~edges_to_delete]\n",
        "        areas = calc_areas_dict_bounded(clipped_vertices_dict)\n",
        "    else:\n",
        "        edges_to_delete = np.full(len(mesh.edges_unique), False)\n",
        "        areas = calc_areas_dict(circumcenters, vertex_faces_dict)\n",
        "        lengths_of_voronoi_edges = torch.zeros((len(mesh.edges_unique)))\n",
        "        lengths_of_voronoi_edges = calc_lengths_of_inner_voronoi_edges(set(list(range(len(mesh.edges_unique)))),\n",
        "                                                                       circumcenters,\n",
        "                                                                       lengths_of_voronoi_edges,\n",
        "                                                                       mesh.edges_unique_faces)\n",
        "\n",
        "    assert len(mesh.edges_unique[~edges_to_delete]) == len(lengths_of_voronoi_edges)\n",
        "    assert len(areas[areas == 0]) == 0\n",
        "\n",
        "    if vizualize:\n",
        "        with torch.no_grad():\n",
        "            fig = plt.figure(figsize=(10, 10))\n",
        "            ax = fig.add_subplot()\n",
        "            if boundary is not None:\n",
        "                for vertex in voronoi_region_to_keep:\n",
        "                    line = clipped_vertices_dict[vertex]\n",
        "                    draw_voronoi_region(coords, line, vertex, ax)\n",
        "                for vertex in voronoi_regions_to_crop:\n",
        "                    line = clipped_vertices_dict[vertex]\n",
        "                    draw_voronoi_region(coords, line, vertex, ax, color='r')\n",
        "            else:\n",
        "                for vertex in vertex_faces_dict.keys():\n",
        "                    line = circumcenters[vertex_faces_dict[vertex]].clone().detach()\n",
        "                    draw_voronoi_region(coords, line, vertex, ax)\n",
        "            plt.show()\n",
        "\n",
        "    return torch.tensor(mesh.edges_unique[~edges_to_delete].T), areas, lengths_of_voronoi_edges, clipped_vertices_dict, voronoi_regions_to_crop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hsXhbDQjWKC"
      },
      "source": [
        "## Solvers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljp3UEpejTG3"
      },
      "outputs": [],
      "source": [
        "class Solver(MessagePassing):\n",
        "    \"\"\"Graph solver class for parabolic equation\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__(aggr='add', flow='target_to_source')\n",
        "\n",
        "    def calc_weights(self, k, e, h, edge_index):\n",
        "        k_i, k_j = k[edge_index[0]], k[edge_index[1]]\n",
        "        numerator = 2 * k_i * k_j * e\n",
        "        denominator = (k_i + k_j) * h + 1e-8\n",
        "        assert numerator.shape == denominator.shape\n",
        "        return torch.divide(numerator, denominator)\n",
        "\n",
        "    def forward(self, u, v, h, e, k, w, p_bhp, dt, edge_index):\n",
        "        assert ~torch.any(torch.isnan(u))\n",
        "        assert ~torch.any(torch.isnan(v))\n",
        "        assert ~torch.any(torch.isnan(h))\n",
        "        assert ~torch.any(torch.isnan(e))\n",
        "        assert ~torch.any(torch.isnan(k))\n",
        "        assert len(u) == len(v), print(len(u), len(v))\n",
        "        assert len(h) == len(e) == edge_index.shape[1]\n",
        "        assert len(u) == len(p_bhp) == len(w) == len(k)\n",
        "        assert torch.all(u >= 0)\n",
        "        assert torch.all(v > 0)\n",
        "        assert torch.all(h > 0)\n",
        "        assert torch.all(e >= 0)\n",
        "        assert torch.all(k > 0)\n",
        "        assert torch.all(w >= 0)\n",
        "        assert torch.all(p_bhp >= 0)\n",
        "\n",
        "\n",
        "        weights = self.calc_weights(k, e, h, edge_index).view(-1, 1)\n",
        "        assert ~torch.any(torch.isnan(weights))\n",
        "        assert len(weights) == edge_index.shape[1]#, print(len(weights), len(edge_index))\n",
        "        edge_index, weights = to_undirected(edge_index, weights)\n",
        "        torch.save(edge_index, 'edge_index.pt')\n",
        "        assert torch.all(weights >= 0)\n",
        "        div = self.propagate(edge_index=edge_index, x=u, weights=weights)\n",
        "        assert len(div) == len(u)\n",
        "        t_over_V = dt / v\n",
        "        numerator = u + t_over_V * (w * p_bhp - div)\n",
        "        denominator = 1 + t_over_V * w\n",
        "        return numerator / denominator\n",
        "\n",
        "    def message(self, x_i, x_j, weights):\n",
        "        du = x_i - x_j\n",
        "        return weights * du\n",
        "\n",
        "def calc_h(coords, edge_index):\n",
        "    x_i, x_j = coords[edge_index[0]], coords[edge_index[1]]\n",
        "    return torch.norm(x_i - x_j, dim=1)\n",
        "\n",
        "def roll_out(solver, n_steps, u0, v, h, e, k, w, p_bhp, dt, edge_index, keep=0, save_animation=False, every_nth=10):\n",
        "    roll = torch.zeros((len(keep), n_steps))\n",
        "    animation=[]\n",
        "    u = u0\n",
        "    for i in range(n_steps):\n",
        "        u = solver(u=u, v=v.view(-1, 1), h=h, e=e, k=k, w=w, p_bhp=p_bhp, dt=dt, edge_index=edge_index)\n",
        "        roll[:, i] = u[keep].squeeze()\n",
        "        if save_animation and i%every_nth==0: animation.append(u.clone().detach())\n",
        "    return roll, animation\n",
        "\n",
        "def dynamics(point_cloud, sources, sinks, boundary, dt=0.0001, n_steps=10000, save_animation=False, every_nth=10):\n",
        "    COORDS = torch.tensor(point_cloud[:, :2], dtype=torch.float32, requires_grad=False)\n",
        "    mesh = triangulate(COORDS.clone().detach())\n",
        "\n",
        "    edge_index, areas, e, _,_ = differentiable_voronoi(COORDS, mesh, boundary)\n",
        "    assert len(COORDS) == len(areas), print(len(COORDS), len(areas), areas)\n",
        "    h = calc_h(COORDS, edge_index)\n",
        "\n",
        "    u0 = torch.zeros((len(COORDS), 1), requires_grad=False)\n",
        "    perm = torch.tensor(point_cloud[:, 2], requires_grad=False)\n",
        "\n",
        "    p_bhp = torch.zeros((len(COORDS), 1), requires_grad=False)\n",
        "    w = torch.zeros((len(COORDS), 1), requires_grad=False)\n",
        "    for s, power in sources.items():\n",
        "        p_bhp[s] = power\n",
        "        w[s] = 1\n",
        "\n",
        "    return roll_out(Solver(), n_steps=n_steps, u0=u0, v=areas, h=h, e=e, k=perm, w=w, p_bhp=p_bhp, dt=dt,\n",
        "                                 edge_index=edge_index, keep=sinks, save_animation=save_animation, every_nth=every_nth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHmig8YPjcAE"
      },
      "outputs": [],
      "source": [
        "def roll_out_1(solver, n_steps, u0, v, h, e, k, w, p_bhp, dt, edge_index, keep=0, save_animation=False, every_nth=10):\n",
        "    roll = torch.zeros((len(u0), n_steps))\n",
        "    animation=[]\n",
        "    u = u0\n",
        "    for i in range(n_steps):\n",
        "\n",
        "        u = solver(u=u, v=v.view(-1, 1), h=h, e=e, k=k, w=w, p_bhp=p_bhp, dt=dt, edge_index=edge_index)\n",
        "        roll[:, i] = u.squeeze()\n",
        "        if save_animation and i%every_nth==0: animation.append(u.clone().detach())\n",
        "    return roll, animation\n",
        "\n",
        "def dynamics_1(point_cloud, sources, sinks, boundary, dt=0.0001, n_steps=10000, save_animation=False, every_nth=10):\n",
        "    COORDS = torch.tensor(point_cloud[:, :2], dtype=torch.float32, requires_grad=False)\n",
        "    mesh = triangulate(COORDS.clone().detach())\n",
        "\n",
        "    edge_index, areas, e, _,_ = differentiable_voronoi(COORDS, mesh, boundary)\n",
        "    assert len(COORDS) == len(areas), print(len(COORDS), len(areas), areas)\n",
        "    h = calc_h(COORDS, edge_index)\n",
        "\n",
        "    u0 = torch.zeros((len(COORDS), 1), requires_grad=False)\n",
        "    perm = torch.tensor(point_cloud[:, 2], requires_grad=False)\n",
        "\n",
        "    p_bhp = torch.zeros((len(COORDS), 1), requires_grad=False)\n",
        "    w = torch.zeros((len(COORDS), 1), requires_grad=False)\n",
        "    for s, power in sources.items():\n",
        "        p_bhp[s] = power\n",
        "        w[s] = 1\n",
        "\n",
        "    return roll_out_1(Solver(), n_steps=n_steps, u0=u0, v=areas, h=h, e=e, k=perm, w=w, p_bhp=p_bhp, dt=dt,\n",
        "                                 edge_index=edge_index, keep=sinks, save_animation=save_animation, every_nth=every_nth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo9k2wc-jeoA"
      },
      "source": [
        "## Learnable coarsener"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KQfXWQ7jlkR"
      },
      "outputs": [],
      "source": [
        "EPS = 1e-15  # Small value to avoid division by zero WITHOUT VOXELIZATION BUT WITH EDGE_WEIGHTS/SOURCES/SINKS\n",
        "\n",
        "\n",
        "class learnableCoarsening(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels: Union[int, List[int]],\n",
        "                 k: int,\n",
        "                 sources: List[int],\n",
        "                 sinks: List[int],\n",
        "                 boundary_nodes: List[int],\n",
        "                 dropout: float = 0.0,\n",
        "                 hidden_channels: int = 3,\n",
        "                 denseconv = False,\n",
        "                 num_layers: int=1,\n",
        "                 temperature: float=1.0,\n",
        "                 gatconv = True,\n",
        "                 seed: int=42\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        if isinstance(channels, int):\n",
        "            channels = [channels]\n",
        "        self.temperature=temperature\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.input_dim = channels[0] + 3\n",
        "        #self.mlp = MLP(channels  + [k], act=\"elu\", norm=None)\n",
        "        #self.mlp = MLP([self.input_dim] + channels[1:] + [k], act=\"elu\", norm=None)\n",
        "        self.mlp = MLP([hidden_channels, k], act=\"elu\", norm=None)\n",
        "        for i in range(num_layers):\n",
        "            in_dim = self.input_dim if i == 0 else hidden_channels\n",
        "            if denseconv:\n",
        "                self.convs.append(DenseGraphConv(in_dim, hidden_channels))\n",
        "                #self.convs.append(GraphConv(in_dim, hidden_channels))\n",
        "            else:\n",
        "                if gatconv:\n",
        "                    self.convs.append(GATConv(in_dim, hidden_channels, edge_dim=1))\n",
        "                else:\n",
        "                    self.convs.append(GraphConv(in_dim, hidden_channels))\n",
        "\n",
        "\n",
        "        self.denseconv=denseconv\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.reset_parameters(seed)\n",
        "        self.intensities = sources\n",
        "        self.sources = torch.tensor(list(sources.keys()), dtype=torch.int)\n",
        "        self.sinks = torch.tensor(sinks, dtype=torch.int)\n",
        "        self.boundary = boundary_nodes\n",
        "        #self.temperature = torch.nn.Parameter(torch.tensor(temperature))\n",
        "\n",
        "\n",
        "\n",
        "    def reset_parameters(self,seed):\n",
        "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
        "        torch.manual_seed(seed)\n",
        "        self.mlp.reset_parameters()\n",
        "        # Custom initialization of layer weights\n",
        "        for conv in self.convs:\n",
        "            for name, param in conv.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    torch.nn.init.xavier_uniform_(param,gain=1.0)\n",
        "                    #torch.nn.init.orthogonal_(param)\n",
        "                    #torch.nn.init.normal_(param, mean=0.0, std=1.0)\n",
        "\n",
        "                elif 'bias' in name:\n",
        "                    #torch.nn.init.zeros_(param)\n",
        "                    torch.nn.init.normal_(param, mean=0.0, std=0.01)\n",
        "\n",
        "\n",
        "        for name, param in self.mlp.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                torch.nn.init.xavier_uniform_(param,gain=2.0)\n",
        "                #torch.nn.init.normal_(param, mean=0.0, std=1.0)\n",
        "\n",
        "            elif 'bias' in name:\n",
        "                #torch.nn.init.zeros_(param)\n",
        "                torch.nn.init.normal_(param, mean=0.0, std=0.01)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Tensor,\n",
        "        adj: Tensor,\n",
        "        edge_weights: Tensor\n",
        "    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
        "        r\"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Node feature tensor\n",
        "                :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times N \\times F}`, with\n",
        "                batch-size :math:`B`, (maximum) number of nodes :math:`N` for\n",
        "                each graph, and feature dimension :math:`F`.\n",
        "                Note that the cluster assignment matrix\n",
        "                :math:`\\mathbf{S} \\in \\mathbb{R}^{B \\times N \\times C}` is\n",
        "                being created within this method.\n",
        "            adj (torch.Tensor): Adjacency tensor\n",
        "\n",
        "        :rtype: (:class:`torch.Tensor`, :class:`torch.Tensor`,\n",
        "            :class:`torch.Tensor`, :class:`torch.Tensor`,\n",
        "        \"\"\"\n",
        "        x = x.unsqueeze(0) if x.dim() == 2 else x\n",
        "        B, N, F = x.size()\n",
        "\n",
        "        # Create masks for sources and sinks\n",
        "        device = x.device\n",
        "        source_mask = torch.zeros(N, 1, device=device) + 1e-8\n",
        "        sink_mask   = torch.zeros(N, 1, device=device) + 1e-8\n",
        "        boundary_mask = torch.zeros(N, 1, device=device) + 1e-8\n",
        "\n",
        "        for key, power in self.intensities.items():\n",
        "            source_mask[key] = power\n",
        "        sink_mask[self.sinks] = 1.0\n",
        "        boundary_mask[self.boundary] = 1.0\n",
        "\n",
        "        # Expand to batch and concatenate with x\n",
        "        source_mask = source_mask.unsqueeze(0).expand(B, -1, -1)\n",
        "        sink_mask = sink_mask.unsqueeze(0).expand(B, -1, -1)\n",
        "        boundary_mask = boundary_mask.unsqueeze(0).expand(B, -1, -1)\n",
        "        x_aug = torch.cat([x, source_mask, sink_mask,boundary_mask], dim=-1)\n",
        "\n",
        "        #x_aug = torch.cat([x, source_mask, sink_mask], dim=-1)\n",
        "\n",
        "        conv_matrix=x_aug[0]\n",
        "        #print(\"x dim:\", x.dim())\n",
        "        #print(x.shape)\n",
        "        for conv in self.convs:\n",
        "            # edge_weights = edge_weights.view(-1, 1)\n",
        "            # conv_matrix=torch.nn.functional.elu(conv(conv_matrix, adj, edge_weights))\n",
        "            if self.denseconv:\n",
        "\n",
        "                conv_matrix = torch.nn.functional.elu(conv(conv_matrix, adj))\n",
        "            else:\n",
        "                edge_weights = edge_weights.view(-1, 1)\n",
        "                conv_matrix = torch.nn.functional.elu(conv(conv_matrix, adj, edge_weights))\n",
        "\n",
        "        s = self.mlp(conv_matrix)\n",
        "        s = s.unsqueeze(0) if s.dim() == 2 else s\n",
        "        #print(s.dim())\n",
        "        s_max = torch.max(s, dim=-1, keepdim=True)[0]  # Max value along the last dimension\n",
        "        s = s - s_max  # Subtract the max value for numerical stability\n",
        "\n",
        "        #print(\"T: \", self.temperature)\n",
        "\n",
        "        s = torch.softmax(s/self.temperature, dim=-1)\n",
        "        sources = torch.zeros((s.shape[1], len(self.sources)))\n",
        "        sources[self.sources, torch.arange(len(self.sources))] = 1.\n",
        "        sinks = torch.zeros((s.shape[1], len(self.sinks)))\n",
        "        sinks[self.sinks, torch.arange(len(self.sinks))] = 1.\n",
        "        s = torch.cat([s, sinks.unsqueeze(0)], dim=-1)\n",
        "        s = torch.cat([s, sources.unsqueeze(0)], dim=-1)\n",
        "\n",
        "        (batch_size, num_nodes, _), C = x.size(), s.size(-1)\n",
        "        '''\n",
        "        s_normed = s.transpose(1, 2) / (torch.sum(s.transpose(1, 2), dim=2, keepdim=True))\n",
        "        out = torch.matmul(s_normed, x)\n",
        "        '''\n",
        "        s_normed = s.transpose(1, 2) / (torch.sum(s.transpose(1, 2), dim=2, keepdim=True))\n",
        "        aggregated = torch.matmul(s_normed, x_aug)  # shape: [B, k, F+2]\n",
        "\n",
        "        out = aggregated\n",
        "\n",
        "\n",
        "        return None, out,None\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.mlp.in_channels}, '\n",
        "                f'num_clusters={self.mlp.out_channels})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwJ7B3FG19Ts"
      },
      "source": [
        "## Stability criterion check and stability loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRSPwWSV12tt"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class StabilityCriterionCheck(MessagePassing):\n",
        "    r\"\"\"\n",
        "    Calculates the real stable time step for the current configuration\n",
        "    of a parabolic equation based on graph-based propagation.\n",
        "\n",
        "    Args:\n",
        "        v (Tensor): cell areas tensor.\n",
        "        k (Tensor): permeability tensor.\n",
        "        e (Tensor): Length of the boundary between Voronoi regions (length of the edge).\n",
        "        h (Tensor): Distance between the centers of Voronoi regions.\n",
        "        edge_index (Tensor): Graph edge indices.\n",
        "\n",
        "    Returns:\n",
        "        float: The computed stable time step.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(aggr='max', flow='target_to_source')\n",
        "\n",
        "    def forward(self, v, k, e, h, edge_index):\n",
        "        v_min = min(v)\n",
        "        k_max = max(k)\n",
        "        C = self.propagate(edge_index=edge_index, e=e, h=h)\n",
        "        C_max = max(C)\n",
        "        return (2 * v_min) / (C_max * k_max)\n",
        "\n",
        "    def message(self, e, h):\n",
        "        return e / (h + 1e-8)\n",
        "\n",
        "\n",
        "\n",
        "class StabilityCriterion(MessagePassing):\n",
        "    r\"\"\"\n",
        "    Calculates the stability loss for the current configuration\n",
        "    of a parabolic equation based on graph-based propagation.\n",
        "\n",
        "    Args:\n",
        "        v (Tensor): cell areas tensor.\n",
        "        k (Tensor): permeability tensor.\n",
        "        e (Tensor): Length of the boundary between Voronoi regions (length of the edge).\n",
        "        h (Tensor): Distance between the centers of Voronoi regions.\n",
        "        edge_index (Tensor): Graph edge indices.\n",
        "\n",
        "    Returns:\n",
        "        float: The computed stability loss.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def smooth_max_min(x,alpha=10.0):\n",
        "        weights = torch.softmax(alpha * x,dim=0)\n",
        "        return (x * weights).sum(dim=0)\n",
        "    def __init__(self, n, s = 1.0):\n",
        "        super().__init__(aggr='max', flow='target_to_source')\n",
        "        self.n = n\n",
        "        self.s = s\n",
        "\n",
        "    def forward(self, v, k, e, h, edge_index):\n",
        "        v_min=self.smooth_max_min(v,alpha=-1.0)\n",
        "        C = self.propagate(edge_index=edge_index, e=e, h=h)\n",
        "        #C=C.squeeze()\n",
        "        C_max=self.smooth_max_min(C,alpha=1.0)\n",
        "        #print(v_min)\n",
        "        #print(C_max)\n",
        "        #return -(4 / (C_max+1e-8)) * (self.n * v_min) / self.s\n",
        "        return -(torch.log(4.0 * self.n * v_min/self.s) - torch.log(C_max))\n",
        "\n",
        "        # out=-4 * (self.n * v_min) / self.s\n",
        "        # out=out.unsqueeze(dim=-1)\n",
        "        # return out\n",
        "\n",
        "    def message(self, e, h):\n",
        "        return e/(h + 1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRnw_AHl2Cev"
      },
      "outputs": [],
      "source": [
        "def ICML_print_grid(ax, COORDS, perm, sources, sinks, clipped_vertices_dict, unit='$K(x, y)$'):\n",
        "    for vertex in clipped_vertices_dict.keys():\n",
        "        line = clipped_vertices_dict[vertex].clone().detach()\n",
        "        cmap = plt.get_cmap('RdYlBu')\n",
        "        p = matplotlib.patches.Polygon(line, facecolor = 'g', edgecolor='black', alpha=0.2, fill=False)\n",
        "        m = torch.mean(torch.unique(line, dim=0), dim=0)\n",
        "        ax.add_patch(p)\n",
        "    scale = ax.scatter(COORDS.detach()[:, 0], COORDS.detach()[:, 1], c=perm.detach(), s=400)\n",
        "    if sources is not None:\n",
        "        for i, src in enumerate(sources):\n",
        "            ax.scatter(COORDS.detach()[src, 0].item(), COORDS.detach()[src, 1].item(), s=100, c='r', marker='v')\n",
        "            ax.text(COORDS.detach()[src, 0].item(), COORDS.detach()[src, 1].item(), 'source ' + str(i+1), fontsize=30)\n",
        "\n",
        "    if sinks is not None:\n",
        "        for i, snk in enumerate(sinks):\n",
        "            ax.scatter(COORDS.detach()[snk, 0].item(), COORDS.detach()[snk, 1].item(), s=100, c='g', marker='^')\n",
        "            ax.text(COORDS.detach()[snk, 0].item(), COORDS.detach()[snk, 1].item(), 'sensor ' + str(i+1), fontsize=30)\n",
        "\n",
        "    cbar = plt.colorbar(scale)\n",
        "    cbar.ax.tick_params(labelsize=24)\n",
        "    cbar.ax.set_ylabel(unit, fontsize=40)\n",
        "\n",
        "def ICML_print(COORDS, clipped_vertices_dict, perm, ground_truth_dynamics, reduced_dynamics, loss_curve,file_title='', title='', sources=None, sinks=None, i=0,save_ani=False,n_steps=1000):\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2,2)\n",
        "    ax1 = fig.add_subplot(gs[:, 0])\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    ax3 = fig.add_subplot(gs[1, 1])\n",
        "\n",
        "    ICML_print_grid(ax1, COORDS, perm, sources, sinks, clipped_vertices_dict)\n",
        "\n",
        "    if reduced_dynamics is not None:\n",
        "        for pp in range(len(sinks)):\n",
        "            ax2.plot(reduced_dynamics[pp], label='reduced_sensor '+str(pp+1))\n",
        "            ax2.plot(ground_truth_dynamics[pp], label='gt_sensor '+str(pp+1))\n",
        "    #ax2.set_title('$p(t)$ at sensor')\n",
        "    ax2.set_xlabel('time step')\n",
        "    ax2.set_ylabel('$p_{s},~Pa$')\n",
        "    ax2.legend()\n",
        "\n",
        "    label_phys = 'phys_loss'\n",
        "    ax3.plot(loss_curve,label = label_phys)\n",
        "    ax3.legend()\n",
        "    ax3.set_yscale('log')\n",
        "    ax3.set_title('Loss')\n",
        "    ax3.set_xlabel('epoch')\n",
        "    ax3.set_ylabel('Loss')\n",
        "    ax1.set_title(title, fontsize=24)\n",
        "    if save_ani:\n",
        "        plt.savefig(f'/content/ani/{str(i).zfill(10)}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iyt7B7Bo2H4M"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "#import neptune\n",
        "def set_seeds(seed=42):\n",
        "    random.seed(seed)  # Random generator python\n",
        "    np.random.seed(seed)  # NumPy\n",
        "    torch.manual_seed(seed)  # PyTorch\n",
        "    torch.cuda.manual_seed_all(seed)  # For all GPU\n",
        "    torch.backends.cudnn.deterministic = True  # Deterministic mode\n",
        "    torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ear1nMFW2LuG"
      },
      "outputs": [],
      "source": [
        "def print_with_neptune(out,dmon, clipped_vertices_dict, point_cloud, boundary,ground_truth_dynamics, reduced_dynamics, adjacency,\n",
        "                            loss_curve,phys_loss_values,stability_loss_values,s, title='', sources=None, sinks=None,run_neptune=False,\n",
        "                            dt=0.001, n_steps=10000, n_epochs=100, clusters=30, channels = [3,8], lr = 0.03,\n",
        "                            warmup_steps = 50, phys_loss_weight = 50,sigmoid_weight=30,savename='test_run',seed=42,unit='$K(x, y)$',i=0,api_token=\"\",phys_start=50,project_name=\"\",num_layers=1,temperature=1.0,denseconv=False):\n",
        "    sources_reduced= list(range(len(out[0]) - len(sources),  len(out[0])))\n",
        "    sinks_reduced= list(range(len(out[0]) - len(sources) - len(sinks),  len(out[0]) - len(sources)))\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2,2)\n",
        "    ax1 = fig.add_subplot(gs[:, 0])\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    ax3 = fig.add_subplot(gs[1, 1])\n",
        "\n",
        "\n",
        "\n",
        "    for vertex in clipped_vertices_dict.keys():\n",
        "        line = clipped_vertices_dict[vertex].clone().detach()\n",
        "        cmap = plt.get_cmap('RdYlBu')\n",
        "        p = matplotlib.patches.Polygon(line, facecolor = 'g', edgecolor='black', alpha=0.2, fill=False)\n",
        "        m = torch.mean(torch.unique(line, dim=0), dim=0)\n",
        "        ax1.add_patch(p)\n",
        "    scale = ax1.scatter(out[0].detach()[:, 0], out[0].detach()[:, 1], c=out[0].detach().numpy()[:, 2], s=400)\n",
        "    if sources is not None:\n",
        "        for j, src in enumerate(sources_reduced):\n",
        "            ax1.scatter(out[0].detach()[src, 0].item(), out[0].detach()[src, 1].item(), s=100, c='r', marker='v')\n",
        "            ax1.text(out[0].detach()[src, 0].item(), out[0].detach()[src, 1].item(), 'source ' + str(j+1), fontsize=30)\n",
        "\n",
        "    if sinks is not None:\n",
        "        for j, snk in enumerate(sinks_reduced):\n",
        "            ax1.scatter(out[0].detach()[snk, 0].item(), out[0].detach()[snk, 1].item(), s=100, c='g', marker='^')\n",
        "            ax1.text(out[0].detach()[snk, 0].item(), out[0].detach()[snk, 1].item(), 'sink ' + str(j+1), fontsize=30)\n",
        "\n",
        "    cbar = plt.colorbar(scale)\n",
        "    cbar.ax.tick_params(labelsize=24)\n",
        "    cbar.ax.set_ylabel('$K(x, y)$', fontsize=40)\n",
        "    if reduced_dynamics is not None:\n",
        "        for j in range(len(sinks)):\n",
        "            ax2.plot(reduced_dynamics[j,:n_steps], label='reduced_sink '+str(j+1))\n",
        "            ax2.plot(ground_truth_dynamics[j,:n_steps], label='gt_sink '+str(j+1))\n",
        "    ax2.set_title('$u(t)$ at sinks')\n",
        "    ax2.set_xlabel('time step')\n",
        "    ax2.legend()\n",
        "\n",
        "\n",
        "    label_stability = 'stability loss'\n",
        "    ax3.plot(stability_loss_values, label=label_stability)\n",
        "    if phys_loss_values is not None:\n",
        "        label_phys = 'phys loss'\n",
        "        ax3.plot(list(range(phys_start, phys_start + len(phys_loss_values))), phys_loss_values, label=label_phys)\n",
        "\n",
        "    ax3.legend()\n",
        "    ax3.set_yscale('log')\n",
        "    ax3.set_title('Loss')\n",
        "    ax3.set_xlabel('epoch')\n",
        "    title=str(i) + ' epoch'\n",
        "    ax1.set_title(title, fontsize=24)\n",
        "\n",
        "    plt.savefig('my_plot.png')\n",
        "    if run_neptune:\n",
        "        run = neptune.init_run(\n",
        "            project=project_name,\n",
        "            api_token=api_token,\n",
        "            name=savename\n",
        "        )\n",
        "        run['parameters'] = {\n",
        "            'learning_rate': lr,\n",
        "            'channels': channels,\n",
        "            'phys_loss_weight': phys_loss_weight,\n",
        "            'n_epochs': n_epochs,\n",
        "            'sigmoid_weight': sigmoid_weight,\n",
        "            'clusters': clusters,\n",
        "            'warmup_steps': warmup_steps,\n",
        "            'field': point_cloud,\n",
        "            'sources': sources,\n",
        "            'sinks': sinks,\n",
        "            'dt': dt,\n",
        "            'boundary': boundary,\n",
        "            'n_steps': n_steps,\n",
        "            'seed':seed,\n",
        "            'num_layers':num_layers,\n",
        "            'temperature':temperature,\n",
        "            'denseconv':denseconv\n",
        "\n",
        "        }\n",
        "        checkpoint = {\n",
        "            'loss_curve': loss_curve,\n",
        "            'adjacency': adjacency,\n",
        "            'optimized_coords': out,\n",
        "            'optimized_dmon': dmon.state_dict(),\n",
        "        }\n",
        "        run['plots/my_plot'].upload('my_plot.png')\n",
        "        torch.save(checkpoint, 'checkpoint1.pth')\n",
        "        run['checkpoints/checkpoint1.pth'].upload('checkpoint1.pth')\n",
        "        run.stop()\n",
        "        print(\"neptune work\")\n",
        "    plt.show()\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnszugdr2PbJ"
      },
      "outputs": [],
      "source": [
        "torch.autograd.set_detect_anomaly(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyTZsdSS71iF"
      },
      "source": [
        "##train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-jami1p2SB_"
      },
      "outputs": [],
      "source": [
        "\n",
        "from matplotlib.patches import Polygon as Polygon_plt\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "def construct_adj_matrix(edge_index):\n",
        "    adj = to_dense_adj(edge_index)\n",
        "    deg = degree(edge_index[0])\n",
        "    deg_sqrt_inv = deg.pow(-0.5)\n",
        "    return deg_sqrt_inv.view(-1, 1) * adj * deg_sqrt_inv.view(1, -1)\n",
        "\n",
        "def calc_h(coords, edge_index):\n",
        "    x_i, x_j = coords[edge_index[0]], coords[edge_index[1]]\n",
        "    return torch.norm(x_i - x_j, dim=1)\n",
        "\n",
        "def calc_physics_weights(k, e, h, edge_index):\n",
        "    k_i, k_j = k[edge_index[0]], k[edge_index[1]]\n",
        "    numerator = 2 * k_i * k_j * e\n",
        "    denominator = (k_i + k_j) * h + 1e-8\n",
        "    assert numerator.shape == denominator.shape\n",
        "    return torch.divide(numerator, denominator)\n",
        "\n",
        "\n",
        "\n",
        "def train(point_cloud, boundary, sources, sinks, ground_truth_dynamics,\n",
        "              dt=0.001, n_steps=10000, n_epochs=100, clusters=30, channels = [3,8], seed=42,\n",
        "              lr = 0.03, warmup_steps = 50, phys_loss_weight = 50,sigmoid_weight=30,savename='test_run',\n",
        "              run_wandb=False,run_neptune=False,api_token=\"\", save_ani = False, denseconv = False,project_name=\"\",num_layers=1,\n",
        "              temperature=1.0,reg_loss_weight=0.0001,n_steps_val=200,n_steps_test=800,stop_bound=55,area_total=1.0, coarsener=None, optimizer=None,\n",
        "              phys_loss_values = None, stability_loss_values = None, reg_loss_values = None, loss_curve = None, curr_epoch = 0,\n",
        "              mse_val=-100.0, do_step=True, orig_mesh = None, orig_edge_index = None, orig_areas = None, orig_e = None, orig_clipped_vertices_dict = None, loss_fn = None, task_field_tensor = None,\n",
        "              orig_h = None, orig_perm = None, orig_edge_weights = None,gatconv=True,orig_boundary_nodes = None):\n",
        "    #start = time.process_time()\n",
        "    #print(ground_truth_dynamics[:,:n_steps].shape[1], n_steps)\n",
        "    assert ground_truth_dynamics[:,:n_steps].shape[1] == n_steps\n",
        "    if task_field_tensor is None:\n",
        "        #print(\"Created tensor\")\n",
        "        FIELD = torch.tensor(point_cloud, requires_grad=False, dtype=torch.float)\n",
        "        #print(FIELD)\n",
        "    else:\n",
        "        print(\"Transferred tensor\")\n",
        "        FIELD = task_field_tensor.detach().contiguous()\n",
        "        #print(FIELD)\n",
        "\n",
        "\n",
        "    if loss_fn is None:\n",
        "        loss_fn = MSELoss()\n",
        "    if loss_curve is None:\n",
        "      loss_curve = []\n",
        "\n",
        "    u0 = torch.zeros((clusters + len(sources) + len(sinks), 1), requires_grad=False)\n",
        "\n",
        "    p_bhp = torch.zeros((clusters + len(sources) + len(sinks), 1), requires_grad=False)\n",
        "    w = torch.zeros((clusters + len(sources) + len(sinks), 1), requires_grad=False)\n",
        "\n",
        "    for i, (_, power) in enumerate(sources.items()):\n",
        "        p_bhp[clusters + len(sinks) + i] = power\n",
        "        w[clusters + len(sinks) + i] = 1.\n",
        "\n",
        "    keep = [i for i in range(clusters, clusters + len(sinks))]\n",
        "    #print(keep)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      if orig_mesh is None:\n",
        "        #print(\"Time before initial differentiable_voronoi call: \", time.process_time() - start)\n",
        "        mesh = triangulate(FIELD[:, :2].clone().detach().numpy())\n",
        "        #print(FIELD.shape, boundary.shape)\n",
        "        edge_index, areas, e, clipped_vertices_dict,boundary_nodes = differentiable_voronoi(FIELD[:, :2], mesh, boundary)\n",
        "        #print(\"Time after initial differentiable_voronoi call: \", time.process_time() - start)\n",
        "        if denseconv:\n",
        "            adjacency = construct_adj_matrix(to_undirected(edge_index))\n",
        "        orig_edge_index = edge_index\n",
        "        perm = FIELD[:, 2]\n",
        "        h = calc_h(FIELD[:, :2], edge_index)\n",
        "        edge_weights = calc_physics_weights(perm, e, h, edge_index)\n",
        "        edge_index, edge_weights = to_undirected(edge_index, edge_weights)\n",
        "\n",
        "\n",
        "      else:\n",
        "        mesh = orig_mesh\n",
        "        edge_index = orig_edge_index\n",
        "        areas = orig_areas\n",
        "        e = orig_e\n",
        "        clipped_vertices_dict = orig_clipped_vertices_dict\n",
        "        if denseconv:\n",
        "            adjacency = construct_adj_matrix(to_undirected(edge_index))\n",
        "        h = orig_h\n",
        "        perm = orig_perm\n",
        "        edge_weights = orig_edge_weights\n",
        "        edge_index, edge_weights = to_undirected(edge_index, edge_weights)\n",
        "        boundary_nodes = orig_boundary_nodes\n",
        "\n",
        "    # print('shape edge_weights ',edge_weights.shape)\n",
        "    # print('shape edge_index ',edge_index.shape)\n",
        "    # edge_index_print, weights_print = to_undirected(edge_index, edge_weights)\n",
        "    # print('shape to_undirected(edge_weights) ',weights_print.shape)\n",
        "    # print('shape to_undirected(edge_index) ',edge_index_print.shape)\n",
        "\n",
        "    if coarsener is None:\n",
        "      coarsener = learnableCoarsening(channels, clusters, sources=sources, sinks=sinks, denseconv=denseconv,num_layers=num_layers,temperature=temperature,seed=seed,gatconv=gatconv,boundary_nodes=list(boundary_nodes))\n",
        "    else:\n",
        "      coarsener.sources = torch.tensor(list(sources.keys()), dtype=torch.int)\n",
        "      coarsener.sinks = torch.tensor(sinks, dtype=torch.int)\n",
        "      coarsener.boundary_nodes = torch.tensor(list(boundary_nodes),dtype=torch.int)\n",
        "    if optimizer is None:\n",
        "      optimizer = torch.optim.Adam(coarsener.parameters(), lr=lr)\n",
        "\n",
        "    solver = Solver()\n",
        "    stability_criterion = StabilityCriterion(n = clusters,s=area_total)\n",
        "    stability_criterion_check = StabilityCriterionCheck()\n",
        "    physical_loss = None\n",
        "    reduced_dynamics = None\n",
        "    if phys_loss_values is None:\n",
        "      phys_loss_values = []\n",
        "    if stability_loss_values is None:\n",
        "      stability_loss_values = []\n",
        "    if reg_loss_values is None:\n",
        "      reg_loss_values=[]\n",
        "\n",
        "    phys_start=warmup_steps\n",
        "\n",
        "    current_mse_val=0.0\n",
        "\n",
        "    qounter_val_stop=0\n",
        "    qounter_bound=stop_bound\n",
        "\n",
        "    #print(weights.shape)\n",
        "\n",
        "    #print(time.process_time() - start)\n",
        "    for i in range(curr_epoch, curr_epoch + n_epochs):\n",
        "        if do_step:\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "        unsqueezed_field = torch.unsqueeze(FIELD, 0)\n",
        "        print(unsqueezed_field.grad)\n",
        "\n",
        "        if denseconv:\n",
        "          s, out, bboxes = coarsener(unsqueezed_field, adjacency, edge_weights)\n",
        "        else:\n",
        "          print('shape edge_weights ',edge_weights.shape)\n",
        "          print('shape edge_index ',edge_index.shape)\n",
        "          s, out, bboxes = coarsener(unsqueezed_field, edge_index, edge_weights)\n",
        "        #print(time.process_time() - start)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            #print(out[0][:, :2].detach().numpy().max(), out[0][:, :2].detach().numpy().min())\n",
        "\n",
        "            mesh = triangulate(out[0][:, :2].clone().detach().numpy())\n",
        "\n",
        "        try:\n",
        "            coarsened_edge_index, coarsened_areas, coarsened_e, coarsened_clipped_vertices_dict,coarsened_boundary_nodes = differentiable_voronoi(out[0][:, :2], mesh, boundary)\n",
        "        except:\n",
        "            print(out[0])\n",
        "        coarsened_h = calc_h(out[0][:, :2], coarsened_edge_index)\n",
        "\n",
        "        edge_index_2, [e_2, h_2] = to_undirected(coarsened_edge_index, [coarsened_e, coarsened_h])\n",
        "        stability_loss = sigmoid_weight*torch.sigmoid(stability_criterion(v=coarsened_areas, k=out[0][:, 2],\n",
        "                                                              e=e_2.view(-1, 1), h=h_2.view(-1, 1),\n",
        "                                                              edge_index=edge_index_2))\n",
        "        # stability_loss = stability_criterion(v=areas, k=out[0][:, 2],\n",
        "        #                                                       e=e_2.view(-1, 1), h=h_2.view(-1, 1),\n",
        "        #                                                       edge_index=edge_index_2)\n",
        "        with torch.no_grad():\n",
        "          max_stable_time = stability_criterion_check(v=coarsened_areas, k=out[0][:, 2],\n",
        "                                                              e=e_2.view(-1, 1), h=h_2.view(-1, 1),\n",
        "                                                              edge_index=edge_index_2)\n",
        "          #print(\"Current max stable time: \", max_stable_time)\n",
        "\n",
        "        if i >= warmup_steps: # after warm-up period we turn on the physical loss, provided that the stability check of the numerical scheme is passed\n",
        "\n",
        "            if max_stable_time > 2*dt: # the condition for the stability of the circuit is fulfilled\n",
        "\n",
        "              reduced_dynamics, _ = roll_out(solver, n_steps=n_steps+n_steps_val, u0=u0, v=coarsened_areas, h=coarsened_h, e=coarsened_e,\n",
        "                                            k=out[0][:,2], w=w, p_bhp=p_bhp, dt=dt,\n",
        "                                            edge_index=coarsened_edge_index, keep=keep)\n",
        "              #print(\"reduced_dynamics shape: \", reduced_dynamics.shape)\n",
        "              assert ground_truth_dynamics.shape == reduced_dynamics.shape\n",
        "              #print(time.process_time() - start)\n",
        "              #physical_loss = torch.mean(torch.sqrt(torch.mean((ground_truth_dynamics[:,:n_steps] - reduced_dynamics[:,:n_steps])**2, dim=1)))/(torch.sqrt(torch.mean(ground_truth_dynamics[:, :n_steps] ** 2)) + 1e-8)*phys_loss_weight\n",
        "              '''\n",
        "                physical_loss = torch.mean(\n",
        "    torch.mean((ground_truth_dynamics[:, :n_steps] - reduced_dynamics[:, :n_steps]) ** 2, dim=1)\n",
        "    / (torch.mean(ground_truth_dynamics[:, :n_steps] ** 2, dim=1) + 1e-8)\n",
        ") * phys_loss_weight\n",
        "            '''\n",
        "              #physical_loss = torch.mean(torch.sqrt(torch.mean((ground_truth_dynamics[:,:n_steps] - reduced_dynamics[:,:n_steps])**2, dim=1))/(torch.sqrt(torch.mean(ground_truth_dynamics[:, :n_steps] ** 2, dim=1)) + 1e-8))*phys_loss_weight\n",
        "              physical_loss = torch.mean(torch.sqrt(torch.mean((ground_truth_dynamics[:,:n_steps] - reduced_dynamics[:,:n_steps])**2, dim=1)))*phys_loss_weight\n",
        "              if n_steps_val>0:\n",
        "                  if mse_val<0:\n",
        "                      mse_val=torch.mean(torch.sqrt(torch.mean((ground_truth_dynamics[:,n_steps:n_steps+n_steps_val] - reduced_dynamics[:,n_steps:n_steps+n_steps_val])**2, dim=1)))\n",
        "\n",
        "                  current_mse_val=torch.mean(torch.sqrt(torch.mean((ground_truth_dynamics[:,n_steps:n_steps+n_steps_val] - reduced_dynamics[:,n_steps:n_steps+n_steps_val])**2, dim=1)))\n",
        "                  if mse_val>current_mse_val:\n",
        "                      mse_val=current_mse_val\n",
        "                      qounter_val_stop=0\n",
        "                  else:\n",
        "                      qounter_val_stop+=1\n",
        "\n",
        "              #print(time.process_time() - start)\n",
        "\n",
        "\n",
        "              phys_loss_values.append(physical_loss.item())\n",
        "\n",
        "\n",
        "              loss = stability_loss[0] + physical_loss\n",
        "              '''\n",
        "              for name, param in coarsener.named_parameters():\n",
        "                print(\"Gradients before backward()\")\n",
        "                print(name, param.grad)\n",
        "              '''\n",
        "\n",
        "              loss.backward()\n",
        "              '''\n",
        "              for name, param in coarsener.named_parameters():\n",
        "                print(\"Gradients after backward()\")\n",
        "                print(name, param.grad)\n",
        "              '''\n",
        "            else: # We adjust the loss for stability until the current dt becomes stable\n",
        "              loss = stability_loss[0]\n",
        "              if len(phys_loss_values) > 0:\n",
        "                phys_loss_values.append(phys_loss_values[len(phys_loss_values) - 1]) # we take it from the previous step\n",
        "              loss.backward()\n",
        "            if len(phys_loss_values)==1:\n",
        "                phys_start=i\n",
        "        else:\n",
        "\n",
        "          loss = stability_loss[0]\n",
        "          loss.backward()\n",
        "        #print(stability_loss[0])\n",
        "        stability_loss_values.append(stability_loss[0].item())\n",
        "\n",
        "        loss_curve.append(loss.item())\n",
        "        #print(time.process_time() - start)\n",
        "\n",
        "        # Apply gradient clipping\n",
        "        if do_step:\n",
        "          nn_utils.clip_grad_norm_(coarsener.parameters(), max_norm=0.2)\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "        if qounter_val_stop>qounter_bound:\n",
        "            break\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            if  i==(n_epochs-1) and run_neptune:\n",
        "                print_with_neptune(out,coarsener, clipped_vertices_dict, point_cloud, boundary,ground_truth_dynamics, reduced_dynamics, adjacency,\n",
        "                            loss_curve,phys_loss_values,stability_loss_values,s, sources=sources, sinks=sinks,run_neptune=run_neptune,\n",
        "                            dt=dt, n_steps=n_steps, n_epochs=n_epochs, clusters=clusters, channels = channels, lr = lr,\n",
        "                            warmup_steps =warmup_steps, phys_loss_weight = phys_loss_weight,sigmoid_weight=sigmoid_weight,\n",
        "                            savename=savename,seed=seed,unit='$K(x, y)$',i=i,api_token=api_token,phys_start=phys_start,project_name=project_name,num_layers=num_layers,temperature=temperature,denseconv=denseconv)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            if i > 1:\n",
        "\n",
        "                if i % 20 == 0 or i==(n_epochs-1):\n",
        "\n",
        "                  clear_output(wait=True)\n",
        "\n",
        "                  sources_reduced= list(range(len(out[0]) - len(sources),  len(out[0])))\n",
        "                  sinks_reduced= list(range(len(out[0]) - len(sources) - len(sinks),  len(out[0]) - len(sources)))\n",
        "                  fig = plt.figure(figsize=(30, 10))\n",
        "                  gs = fig.add_gridspec(2,2)\n",
        "                  ax1 = fig.add_subplot(gs[:, 0])\n",
        "                  ax2 = fig.add_subplot(gs[0, 1])\n",
        "                  ax3 = fig.add_subplot(gs[1, 1])\n",
        "\n",
        "\n",
        "\n",
        "                  for vertex in coarsened_clipped_vertices_dict.keys():\n",
        "                      line = coarsened_clipped_vertices_dict[vertex].clone().detach()\n",
        "                      cmap = plt.get_cmap('RdYlBu')\n",
        "                      p = matplotlib.patches.Polygon(line, facecolor = 'g', edgecolor='black', alpha=0.2, fill=False)\n",
        "                      m = torch.mean(torch.unique(line, dim=0), dim=0)\n",
        "                      ax1.add_patch(p)\n",
        "                  scale = ax1.scatter(out[0].detach()[:, 0], out[0].detach()[:, 1], c=out[0].detach().numpy()[:, 2], s=400)\n",
        "                  if sources is not None:\n",
        "                      for j, src in enumerate(sources_reduced):\n",
        "                          ax1.scatter(out[0].detach()[src, 0].item(), out[0].detach()[src, 1].item(), s=100, c='r', marker='v')\n",
        "                          ax1.text(out[0].detach()[src, 0].item(), out[0].detach()[src, 1].item(), 'source ' + str(j+1), fontsize=30)\n",
        "\n",
        "                  if sinks is not None:\n",
        "                      for j, snk in enumerate(sinks_reduced):\n",
        "                          ax1.scatter(out[0].detach()[snk, 0].item(), out[0].detach()[snk, 1].item(), s=100, c='g', marker='^')\n",
        "                          ax1.text(out[0].detach()[snk, 0].item(), out[0].detach()[snk, 1].item(), 'sensor ' + str(j+1), fontsize=30)\n",
        "                  '''\n",
        "                  k = len(windows)\n",
        "                  cmap = plt.get_cmap('tab20', k)  #    c k \n",
        "\n",
        "                  for idx, poly in enumerate(windows):\n",
        "                      pts = poly.cpu().numpy()  #  Tensor  numpy\n",
        "                      color = cmap(idx)\n",
        "                      patch = Polygon_plt(\n",
        "                          pts,\n",
        "                          closed=True,\n",
        "                          edgecolor=color,\n",
        "                          facecolor='none',\n",
        "                          linestyle='--',    # \n",
        "                          linewidth=2,\n",
        "                          alpha=0.8\n",
        "                      )\n",
        "                      ax1.add_patch(patch)\n",
        "                  '''\n",
        "\n",
        "                  cbar = plt.colorbar(scale)\n",
        "                  cbar.ax.tick_params(labelsize=24)\n",
        "                  cbar.ax.set_ylabel('$K(x, y)$', fontsize=40)\n",
        "\n",
        "                  if reduced_dynamics is not None:\n",
        "                      for pp in range(len(sinks_reduced)):\n",
        "                          ax2.plot(reduced_dynamics[pp,:n_steps], label='reduced_sensor '+str(pp+1))\n",
        "                          ax2.plot(ground_truth_dynamics[pp,:n_steps], label='gt_sensor '+str(pp+1))\n",
        "                  #ax2.set_title('$u(t)$ at sinks')\n",
        "                  ax2.set_xlabel('time step')\n",
        "                  ax2.set_ylabel('$p_{s},~Pa$')\n",
        "                  ax2.legend()\n",
        "\n",
        "                  label_stability = 'stability loss'\n",
        "                  ax3.plot(stability_loss_values, label=label_stability)\n",
        "                  if phys_loss_values is not None:\n",
        "                      label_phys = 'phys loss'\n",
        "                      ax3.plot(list(range(phys_start, phys_start + len(phys_loss_values))), phys_loss_values, label=label_phys)\n",
        "\n",
        "                  ax3.legend()\n",
        "                  ax3.set_yscale('log')\n",
        "                  ax3.set_title('Loss')\n",
        "                  ax3.set_xlabel('epoch')\n",
        "                  ax3.set_ylabel('Loss')\n",
        "                  title=str(i) + ' epoch'\n",
        "                  ax1.set_title(title, fontsize=24)\n",
        "\n",
        "                  if save_ani:\n",
        "                    plt.savefig(f'/content/ani/{str(i).zfill(10)}')\n",
        "                  plt.show()\n",
        "\n",
        "                  #sleep(4)\n",
        "\n",
        "                  #clear_output(wait=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return loss_curve, coarsener, 0, out.detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt0gGxUT3BLN"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def train_competing(point_cloud, boundary, sources, sinks, ground_truth_dynamics, clamp_x = [0, 1.0], clamp_y = [0, 1.0], dt=0.001, n_steps=10000, n_epochs=100,save_ani=False):\n",
        "    assert ground_truth_dynamics.shape[1] == n_steps\n",
        "    COORDS = torch.tensor(point_cloud[:, :2], requires_grad=True)\n",
        "    optimizer = torch.optim.Adam([COORDS], lr=0.002)\n",
        "    loss_fn = MSELoss()\n",
        "    loss_curve = []\n",
        "    mesh = triangulate(COORDS.clone().detach())\n",
        "\n",
        "    mask = torch.zeros((len(COORDS), 1))\n",
        "    u0 = torch.zeros((len(COORDS), 1), requires_grad=False)\n",
        "    perm = torch.tensor(point_cloud[:, 2], requires_grad=False)\n",
        "\n",
        "    p_bhp = torch.zeros((len(COORDS), 1), requires_grad=False)\n",
        "    w = torch.zeros((len(COORDS), 1), requires_grad=False)\n",
        "    for s, power in sources.items():\n",
        "        p_bhp[s] = power\n",
        "        w[s] = 1\n",
        "        mask[s] = 1\n",
        "    for s in sinks:\n",
        "        mask[s] = 1\n",
        "\n",
        "    solver = Solver()\n",
        "\n",
        "    stability_criterion_check = StabilityCriterionCheck()\n",
        "\n",
        "    for i in range(1, n_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        C_n = COORDS.clone().detach()\n",
        "\n",
        "        edge_index, areas, e, clipped_vertices_dict,_ = differentiable_voronoi(COORDS, mesh, boundary)\n",
        "        h = calc_h(COORDS, edge_index)\n",
        "        edge_index_2, [e_2, h_2] = to_undirected(edge_index, [e, h])\n",
        "        with torch.no_grad():\n",
        "          max_stable_time = stability_criterion_check(v=areas, k=point_cloud[:, 2],\n",
        "                                                                e=e_2.view(-1, 1), h=h_2.view(-1, 1),\n",
        "                                                                edge_index=edge_index_2)\n",
        "          #print(\"Current max stable time: \", max_stable_time)\n",
        "        reduced_dynamics, _ = roll_out(solver, n_steps=n_steps, u0=u0, v=areas, h=h, e=e, k=perm, w=w, p_bhp=p_bhp, dt=dt,\n",
        "                                    edge_index=edge_index, keep=sinks)\n",
        "\n",
        "        loss = torch.mean(torch.sqrt(torch.mean((ground_truth_dynamics - reduced_dynamics)**2, dim=1)))\n",
        "        loss_curve.append(loss.item())\n",
        "        #print(i, loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            COORDS[:] = COORDS - mask * (COORDS - C_n)\n",
        "            COORDS[:, 0] = COORDS[:, 0].clamp(clamp_x[0], clamp_x[1])\n",
        "            COORDS[:, 1] = COORDS[:, 1].clamp(clamp_y[0], clamp_y[1])\n",
        "            mesh = triangulate(COORDS.clone().detach())\n",
        "\n",
        "            if i % 5 == 0:\n",
        "                clear_output(wait=True)\n",
        "                #print(i, loss.item())\n",
        "                print(\"Before printing: \", COORDS[:,0].max(), COORDS[:,1].min())\n",
        "                ICML_print(COORDS, clipped_vertices_dict, perm, ground_truth_dynamics, reduced_dynamics, loss_curve,\n",
        "                           sources=sources, sinks=sinks, title=str(i) + ' epoch', file_title = datetime.now().strftime('%Y-%m-%d_%H-%M-%S'), i=i,save_ani=save_ani)\n",
        "\n",
        "    result_field = torch.cat((COORDS, torch.from_numpy(point_cloud)[:, 2].unsqueeze(1)), dim=1)\n",
        "\n",
        "    return loss_curve, result_field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFYXQLpH3Ftf"
      },
      "outputs": [],
      "source": [
        "def reduce_graph(graph, k, sources=[0], sinks=[1]):\n",
        "    labels = KMeans(k, random_state=17).fit_predict(graph[:, :2])\n",
        "    loop_reduced = avg_pool_x(torch.tensor(labels), torch.tensor(graph), torch.ones(len(labels)))[0]\n",
        "    loop_reduced = torch.cat([loop_reduced, torch.tensor(graph[list(sources.keys())]), torch.tensor(graph[sinks])], dim=0)\n",
        "    loop_reduced, return_inverse, _ = torch.unique(loop_reduced, return_inverse=True, return_counts=True,  dim=0)\n",
        "    loop_reduced_saved = loop_reduced\n",
        "\n",
        "    for i, p in enumerate(loop_reduced):\n",
        "        for j, n in enumerate(loop_reduced):\n",
        "            if i != j and torch.norm(p - n) < 1e-4:\n",
        "                raise ValueError\n",
        "\n",
        "    return loop_reduced.detach().numpy(), return_inverse[-len(sources) - len(sinks): - len(sinks)], return_inverse[-len(sinks):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhE_0nzW6kka"
      },
      "outputs": [],
      "source": [
        "def try_different_degrees_of_reduction(orig_field, boundary, dt, n_steps, reduction_degrees=[0.75, 0.5, 0.25],\n",
        "                                                    n_epochs=100, aggregating_function=KMeans, method='gnn',\n",
        "                                                    sources = None, sinks = None,clusters=30, channels = [3,8], seed=42,\n",
        "                                                    lr = 0.03, warmup_steps = 50, phys_loss_weight = 50,sigmoid_weight=30\n",
        "                                                    ,savename='test_run',run_wandb=False,run_neptune=False,clamp_x = [0, 10.0], clamp_y = [0, 10.0], api_token=\"\",\n",
        "                                                    source_power = 100.0, denseconv=False, save_ani=False,temperature=1.0,num_layers=1,project_name=\"\",n_steps_val=0,stop_bound=55,top_k=1.0,gatconv = True):\n",
        "    ground_truth_dynamics, ani = dynamics(orig_field, sources=sources,\n",
        "                             sinks=sinks, boundary=boundary,\n",
        "                             dt=dt, save_animation=True, n_steps=n_steps+n_steps_val)\n",
        "    print(len(ground_truth_dynamics[:,:n_steps]))\n",
        "    N = len(orig_field)\n",
        "\n",
        "    storage = {}\n",
        "    storage[1] = ground_truth_dynamics[:,:n_steps]\n",
        "    coarseners = {}\n",
        "    for r in reduction_degrees:\n",
        "        n = int(r * N)\n",
        "        assert n > 0\n",
        "        if method == 'points' or method == 'kmeans':\n",
        "          reduced_point_cloud, sources_reduced, sinks_reduced = reduce_graph(orig_field, n, sources = sources, sinks = sinks) # k-means (initial)\n",
        "          print(\"sources_reduced\",sources_reduced,\"sinks_reduced\",sinks_reduced)\n",
        "          sources_reduced = {src: source_power for src in sources_reduced}\n",
        "\n",
        "        if method != 'kmeans':\n",
        "          if method == 'points':\n",
        "            loss_curve, reduced_point_cloud = train_competing(reduced_point_cloud, boundary,\n",
        "                                    {src: source_power for src in sources_reduced},sinks_reduced, ground_truth_dynamics[:,:n_steps],\n",
        "                                    clamp_x = clamp_x, clamp_y = clamp_y, dt=dt, n_steps=n_steps,\n",
        "                                    n_epochs=n_epochs,save_ani=save_ani)\n",
        "          elif method == 'gnn':\n",
        "            loss_curve, coarsener, adjacency, reduced_point_cloud = train(orig_field,\n",
        "                  sources=sources,\n",
        "                  sinks=sinks,\n",
        "                  ground_truth_dynamics=ground_truth_dynamics,\n",
        "                  boundary=boundary,\n",
        "                  dt=dt, n_steps=n_steps, n_epochs=n_epochs,clusters=n, channels = channels, seed=seed,\n",
        "                  lr = lr, warmup_steps = warmup_steps, phys_loss_weight = phys_loss_weight,\n",
        "                  sigmoid_weight=sigmoid_weight,savename=savename,run_wandb=run_wandb, run_neptune=run_neptune,api_token=api_token,project_name=project_name,\n",
        "                  denseconv=denseconv, save_ani=save_ani,temperature=temperature,num_layers=num_layers,n_steps_val=n_steps_val,stop_bound=stop_bound,gatconv=gatconv)\n",
        "            reduced_point_cloud = reduced_point_cloud[0]\n",
        "\n",
        "            sources_reduced = list(range(len(reduced_point_cloud) - len(sources),  len(reduced_point_cloud)))\n",
        "            sources_reduced = {src: source_power for src in sources_reduced}\n",
        "            sinks_reduced = list(range(len(reduced_point_cloud) - len(sources) - len(sinks),  len(reduced_point_cloud) - len(sources)))\n",
        "            print(sources_reduced, sinks_reduced)\n",
        "            coarseners[r]=coarsener\n",
        "\n",
        "        reduced_dynamics, _ = dynamics(reduced_point_cloud, sources_reduced,sinks_reduced, boundary, n_steps=3*n_steps, dt = dt)\n",
        "\n",
        "        storage[r] = reduced_dynamics, reduced_point_cloud\n",
        "\n",
        "    if method == \"gnn\":\n",
        "      return storage, coarseners\n",
        "    else:\n",
        "      return storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg8Xupur6uiJ"
      },
      "source": [
        "## Sinusoidal variable permeability scenario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9FSHqHh6m-8"
      },
      "outputs": [],
      "source": [
        "l = 1\n",
        "x, y = np.meshgrid(np.linspace(0, l, 20), np.linspace(0, l, 20))\n",
        "x = x.flatten().reshape(-1, 1)\n",
        "y = y.flatten().reshape(-1, 1)\n",
        "k = np.cos(20 * x)+ np.sin(y)\n",
        "k = k + np.abs(np.min(k)) + 2.0\n",
        "eps = 1e-1\n",
        "task1 = {'field': np.concatenate([x, y, k], axis=1),\n",
        "        'sources': {src: 100. for src in [210]},\n",
        "        'sinks': [42, 399-42],\n",
        "        'dt': 0.00001,\n",
        "        'boundary': torch.tensor([[-eps,-eps],\n",
        "                                  [-eps, l + eps],\n",
        "                                   [l + eps, l + eps],\n",
        "                                    [l + eps, -eps]], requires_grad=False),\n",
        "        'n_steps': 1000}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scatter = plt.scatter(task1['field'][:, 0], task1['field'][:, 1], c=task1['field'][:, 2])\n",
        "#plt.title(\"Original permeability field (cos(20x) + sin(y))\")\n",
        "# s = list(task1['sources'].keys())\n",
        "# plt.scatter(x[s], y[s], marker='D', c='m')\n",
        "ii=0\n",
        "for j in task1['sinks']:\n",
        "  plt.scatter(task1['field'][j,0], task1['field'][j,1], s=100, c='g', marker='^')\n",
        "  plt.annotate('sensor'+str(ii+1), (task1['field'][j,0], task1['field'][j,1]),xytext=(-20.0, 0),textcoords='offset points', fontsize=16)\n",
        "  ii+=1\n",
        "\n",
        "ii=0\n",
        "for j in task1['sources'].keys():\n",
        "  plt.scatter(task1['field'][j,0], task1['field'][j,1], s=100, c='r', marker='v')\n",
        "  plt.annotate('source'+str(ii+1), (task1['field'][j,0], task1['field'][j,1]), fontsize=16)\n",
        "  ii+=1\n",
        "plt.colorbar(scatter)"
      ],
      "metadata": {
        "id": "-wk6BN2cN5p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth, ani = dynamics(task1['field'], sources=task1['sources'],\n",
        "                             sinks=task1['sinks'], boundary=task1['boundary'],\n",
        "                             dt=task1['dt'], save_animation=False, n_steps=task1['n_steps'])"
      ],
      "metadata": {
        "id": "X5p6EoxGfAp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzBMuhxj6sqB"
      },
      "outputs": [],
      "source": [
        "set_seeds(42)\n",
        "storage_01, coarsener_01 = try_different_degrees_of_reduction(task1['field'], task1['boundary'], task1['dt'], task1['n_steps'],\n",
        "                                                        reduction_degrees=[0.1], n_epochs=300, lr = 0.02, sigmoid_weight = 0.1, phys_loss_weight = 100.0,\n",
        "                                                        aggregating_function=KMeans, method='gnn', sources = task1['sources'], sinks = task1['sinks'], save_ani=False,\n",
        "                                                        denseconv=False, num_layers=3,temperature=0.4,n_steps_val=0,warmup_steps=50,gatconv = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvMZFTXh9GMx"
      },
      "outputs": [],
      "source": [
        "set_seeds(42)\n",
        "storage_0075, coarsener_0075 = try_different_degrees_of_reduction(task1['field'], task1['boundary'], task1['dt'], task1['n_steps'],\n",
        "                                                        reduction_degrees=[0.075], n_epochs=300, lr = 0.008, sigmoid_weight = 0.1, phys_loss_weight = 100.0,\n",
        "                                                        aggregating_function=KMeans, method='gnn', sources = task1['sources'], sinks = task1['sinks'], save_ani=False,\n",
        "                                                        denseconv=False, num_layers=3,temperature=0.4,n_steps_val=0,warmup_steps=50,gatconv = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVYqPy6L-eIs"
      },
      "outputs": [],
      "source": [
        "set_seeds(42)\n",
        "storage_005, coarsener_005 = try_different_degrees_of_reduction(task1['field'], task1['boundary'], task1['dt'], task1['n_steps'],\n",
        "                                                        reduction_degrees=[0.05], n_epochs=300, lr = 0.01, sigmoid_weight = 0.1, phys_loss_weight = 100.0,\n",
        "                                                        aggregating_function=KMeans, method='gnn', sources = task1['sources'], sinks = task1['sinks'], save_ani=False,\n",
        "                                                        denseconv=False, num_layers=3,temperature=0.4,n_steps_val=0,warmup_steps=50,gatconv = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWXekbt4GbPD"
      },
      "outputs": [],
      "source": [
        "path='/content/gdrive/MyDrive/ML_ST/new_storage_sin_01.pt'\n",
        "torch.save(storage_01, path)\n",
        "\n",
        "path='/content/gdrive/MyDrive/ML_ST/new_storage_sin_0075.pt'\n",
        "torch.save(storage_0075, path)\n",
        "\n",
        "path='/content/gdrive/MyDrive/ML_ST/new_storage_sin_005.pt'\n",
        "torch.save(storage_005, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rprV_hBNZN_U"
      },
      "outputs": [],
      "source": [
        "path='/content/gdrive/MyDrive/ML_ST/new_coarsener_sin_01.pt'\n",
        "torch.save(coarsener_01, path)\n",
        "\n",
        "path='/content/gdrive/MyDrive/ML_ST/new_coarsener_sin_0075.pt'\n",
        "torch.save(coarsener_0075, path)\n",
        "\n",
        "path='/content/gdrive/MyDrive/ML_ST/new_coarsener_sin_005.pt'\n",
        "torch.save(coarsener_005, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HonfjLjbTyS"
      },
      "outputs": [],
      "source": [
        "storage_01 = torch.load('/content/gdrive/MyDrive/ML_ST/new_storage_sin_01.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJuD5ZrPSFiT"
      },
      "source": [
        "##Metis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qie9bhqSFIP"
      },
      "outputs": [],
      "source": [
        "def metis_coarsening_sin(path,n_clusters):\n",
        "    data = []\n",
        "    with open(path, \"r\") as file:\n",
        "        for line in file:\n",
        "            row = [float(x) for x in line.split()]\n",
        "            row[-1] = int(row[-1])\n",
        "            data.append(row)\n",
        "\n",
        "    data = np.array(data)\n",
        "\n",
        "    data[42][3]=n_clusters+1\n",
        "    data[399-42][3]=n_clusters+2\n",
        "    data[210][3]=n_clusters+3\n",
        "\n",
        "    clusters = np.unique(data[:, 3])\n",
        "    centers_of_mass = {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for cluster in clusters:\n",
        "        points = data[data[:, 3] == cluster][:, :3]\n",
        "        center=[0,0,0]\n",
        "        points[:,0]=points[:,0]*points[:,2]\n",
        "        points[:,1]=points[:,1]*points[:,2]\n",
        "        center[0]=points[:,0].sum()/points[:,2].sum()\n",
        "        center[1]=points[:,1].sum()/points[:,2].sum()\n",
        "        center[2]=points[:,2].mean()\n",
        "        centers_of_mass[cluster] = center\n",
        "\n",
        "    centers_xy=[]\n",
        "    for cluster_id, center in centers_of_mass.items():\n",
        "        centers_xy.append(center)\n",
        "\n",
        "    #centers_xy.append(field[180])\n",
        "    #centers_xy.append(field[0])\n",
        "\n",
        "    centers_xy=np.array(centers_xy)\n",
        "\n",
        "    x = centers_xy[:,0].reshape(-1, 1)\n",
        "    y = centers_xy[:,1].reshape(-1, 1)\n",
        "    k = centers_xy[:,2].reshape(-1, 1)\n",
        "    eps = 1e-1\n",
        "    task2 = {'field': np.concatenate([x, y, k], axis=1),\n",
        "            'sources': {src: 100. for src in [n_clusters+2]},\n",
        "            'sinks': [n_clusters,n_clusters+1],\n",
        "            'dt': 0.00001,\n",
        "            'boundary': torch.tensor([[-eps,-eps],\n",
        "                                      [-eps, l + eps],\n",
        "                                      [l + eps, l + eps],\n",
        "                                        [l + eps, -eps]], requires_grad=False),\n",
        "            'n_steps': 1000}\n",
        "\n",
        "    return task2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rv5luW_bSI_U"
      },
      "outputs": [],
      "source": [
        "metis_sin_01_field=metis_coarsening_sin('/content/output_points_sin_40.txt',40)\n",
        "metis_sin_0075_field=metis_coarsening_sin('/content/output_points_sin_30.txt',30)\n",
        "metis_sin_005_field=metis_coarsening_sin('/content/output_points_sin_20.txt',20)\n",
        "\n",
        "metis_dynamics_01_sin, ani = dynamics(metis_sin_01_field['field'], sources=metis_sin_01_field['sources'],\n",
        "                             sinks=metis_sin_01_field['sinks'], boundary=task1['boundary'],\n",
        "                             dt=task1['dt'], save_animation=False, n_steps=3*task1['n_steps'])\n",
        "metis_dynamics_0075_sin, ani = dynamics(metis_sin_0075_field['field'], sources=metis_sin_0075_field['sources'],\n",
        "                             sinks=metis_sin_0075_field['sinks'], boundary=metis_sin_0075_field['boundary'],\n",
        "                             dt=metis_sin_0075_field['dt'], save_animation=False, n_steps=3*metis_sin_0075_field['n_steps'])\n",
        "metis_dynamics_005_sin, ani = dynamics(metis_sin_005_field['field'], sources=metis_sin_005_field['sources'],\n",
        "                             sinks=metis_sin_005_field['sinks'], boundary=task1['boundary'],\n",
        "                             dt=task1['dt'], save_animation=False, n_steps=3*task1['n_steps'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FH_QsGpRajk"
      },
      "source": [
        "##Load sinusoidal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZCwzYmETBQv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xuk1a22vRelI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W553pL-TCeB"
      },
      "outputs": [],
      "source": [
        "# path='/content/gdrive/MyDrive/icml_2025/new_storage_sin_01_val_200.pt'\n",
        "# storage_01 = torch.load(path)\n",
        "\n",
        "# path='/content/gdrive/MyDrive/icml_2025/new_storage_sin_005_val_500.pt'\n",
        "# storage_005 = torch.load(path)\n",
        "\n",
        "# path='/content/gdrive/MyDrive/icml_2025/new_storage_sin_0075_val_200.pt'\n",
        "# storage_0075 = torch.load(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHe92aoTV1wM"
      },
      "outputs": [],
      "source": [
        "result_competing_method=torch.load('/content/gdrive/MyDrive/icml_2025/result_competing_method_sinusoidal_lr_0.005.pt')\n",
        "#result_competing_method=torch.load('/content/gdrive/MyDrive/result_competing_method_sinusoidal_lr_0.005.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKfH5hj8xFqP"
      },
      "outputs": [],
      "source": [
        "#result_competing_method[0.05]=torch.load('/content/result_old_method_0.05_sinks_2_sources_1.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87QOB1JTxioT"
      },
      "outputs": [],
      "source": [
        "x = torch.load('/content/result_old_method_0.05_sinks_2_sources_1.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "043yRbWVAEPw"
      },
      "outputs": [],
      "source": [
        "storage_01 = torch.load('/content/gdrive/MyDrive/ML_ST/new_storage_sin_01.pt')\n",
        "storage_0075 = torch.load('/content/gdrive/MyDrive/ML_ST/new_storage_sin_0075.pt')\n",
        "storage_005 = torch.load('/content/gdrive/MyDrive/ML_ST/new_storage_sin_005.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x44UoRd8SZYi"
      },
      "source": [
        "##Printing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg2_Hgx8SbPr"
      },
      "outputs": [],
      "source": [
        "def ICLR_compare_rmse_2(stores, labels,start,end):\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "    markers = iter([\"o\", \"s\", \"D\"])\n",
        "    for i, res in enumerate(stores):\n",
        "        errors = {}\n",
        "        gt = res[1][start:end]\n",
        "        for k, v in res.items():\n",
        "            if k == 1: continue\n",
        "            error = np.log10(np.sqrt(mean_squared_error(gt, v[start:end])))\n",
        "            errors[k] = error\n",
        "        errors = dict(sorted(errors.items()))\n",
        "        ax.scatter(list(errors.keys()), list(errors.values()), label=labels[i], marker=next(markers), s=200)\n",
        "    plt.legend(fontsize=20, loc='upper right')\n",
        "    ax.set_ylabel('RMSE, log10 scale', fontsize=20)\n",
        "    ax.set_xlabel('r', fontsize=20)\n",
        "    plt.xticks(fontsize=20)\n",
        "    plt.yticks(fontsize=20)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fySMjfs-Sh7s"
      },
      "outputs": [],
      "source": [
        "def ICLR_compare_curves_2(result,end):\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "    for k, v in result.items():\n",
        "        ax.plot(v[:end], label='r=' + str(round(k, 3)), linewidth=4)\n",
        "    plt.legend(fontsize=20)\n",
        "    ax.set_ylabel('$p_{s},~Pa$', fontsize=20)\n",
        "    ax.set_xlabel('time steps', fontsize=20)\n",
        "    plt.xticks(fontsize = 20)\n",
        "    plt.yticks(fontsize = 20)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YPe3hf4SkIE"
      },
      "outputs": [],
      "source": [
        "def ICLR_compare_rmse_2(stores, labels,start,end):\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "    markers = iter([\"o\", \"s\", \"D\",\"^\"])\n",
        "    for i, res in enumerate(stores):\n",
        "        errors = {}\n",
        "        gt = res[1][start:end]\n",
        "        for k, v in res.items():\n",
        "            if k == 1: continue\n",
        "            error = np.log10(np.sqrt(mean_squared_error(gt, v[start:end])))\n",
        "            errors[k] = error\n",
        "        errors = dict(sorted(errors.items()))\n",
        "        ax.scatter(list(errors.keys()), list(errors.values()), label=labels[i], marker=next(markers), s=200)\n",
        "        #ax.scatter(list(errors.keys()), list(errors.values()), label=labels[i], s=200)\n",
        "    plt.legend(fontsize=20, loc='upper right')\n",
        "    ax.set_ylabel('RMSE, log10 scale', fontsize=20)\n",
        "    ax.set_xlabel('r', fontsize=20)\n",
        "    plt.xticks(fontsize=20)\n",
        "    plt.yticks(fontsize=20)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvzOLKsmSqTF"
      },
      "outputs": [],
      "source": [
        "result_kmeans = try_different_degrees_of_reduction(task1['field'], task1['boundary'], task1['dt'], task1['n_steps'], reduction_degrees=[0.1, 0.075, 0.05], n_epochs=300,\n",
        "                                                   lr = 0.1, aggregating_function=KMeans, method='kmeans', sources = task1['sources'], sinks = task1['sinks'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZATkUJvWWEuZ"
      },
      "outputs": [],
      "source": [
        "l = 1\n",
        "x, y = np.meshgrid(np.linspace(0, l, 20), np.linspace(0, l, 20))\n",
        "x = x.flatten().reshape(-1, 1)\n",
        "y = y.flatten().reshape(-1, 1)\n",
        "k = np.cos(20 * x)+ np.sin(y)\n",
        "k = k + np.abs(np.min(k)) + 2.0\n",
        "eps = 1e-1\n",
        "task1 = {'field': np.concatenate([x, y, k], axis=1),\n",
        "        'sources': {src: 100. for src in [210]},\n",
        "        'sinks': [42, 399-42],\n",
        "        'dt': 0.00001,\n",
        "        'boundary': torch.tensor([[-eps,-eps],\n",
        "                                  [-eps, l + eps],\n",
        "                                   [l + eps, l + eps],\n",
        "                                    [l + eps, -eps]], requires_grad=False),\n",
        "        'n_steps': 1000}\n",
        "\n",
        "ground_truth_2000, ani = dynamics(task1['field'], sources=task1['sources'],\n",
        "                             sinks=task1['sinks'], boundary=task1['boundary'],\n",
        "                             dt=task1['dt'], save_animation=False, n_steps=3*task1['n_steps'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvCfWs2fS2Sv"
      },
      "outputs": [],
      "source": [
        "orig_curve = ground_truth_2000\n",
        "\n",
        "result_new_01 = storage_01[0.1]\n",
        "result_new_005 = storage_005[0.05]\n",
        "result_new_0075 = storage_0075[0.075]\n",
        "\n",
        "result_competing_01 = result_competing_method[0.1]\n",
        "result_competing_005 = result_competing_method[0.05]\n",
        "result_competing_0075 =  result_competing_method[0.075]\n",
        "\n",
        "result_kmeans_01 = result_kmeans[0.1]\n",
        "result_kmeans_005 =result_kmeans[0.05]\n",
        "result_kmeans_0075 = result_kmeans[0.075]\n",
        "\n",
        "result_new = {}\n",
        "result_new[1] = orig_curve[0]\n",
        "result_new[0.1] = result_new_01[0][0].tolist()\n",
        "result_new[0.05] = result_new_005[0][0].tolist()\n",
        "result_new[0.075] = result_new_0075[0][0].tolist()\n",
        "result_competing = {}\n",
        "result_competing[1] = orig_curve[0]\n",
        "result_competing[0.1] = result_competing_01[0][0].tolist()\n",
        "result_competing[0.05] = result_competing_005[0][0].tolist()\n",
        "result_competing[0.075] = result_competing_0075[0][0].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJx17t_LSBUO"
      },
      "outputs": [],
      "source": [
        "# result_new = {}\n",
        "# result_new[1] = orig_curve[0]\n",
        "# result_new[0.1] = result_new_01[0][0].tolist()\n",
        "# result_new[0.05] = result_new_005[0][0].tolist()\n",
        "# result_new[0.075] = result_new_0075[0][0].tolist()\n",
        "# result_competing = {}\n",
        "# result_competing[1] = orig_curve[0]\n",
        "# result_competing[0.1] = result_competing_01[0][0].tolist()\n",
        "# result_competing[0.05] = result_competing_005[0][0].tolist()\n",
        "# result_competing[0.075] = result_competing_0075[0][0].tolist()\n",
        "\n",
        "result_kmeans_ = {}\n",
        "result_kmeans_[1] = orig_curve[0]\n",
        "result_kmeans_[0.1] = result_kmeans_01[0][0].tolist()\n",
        "result_kmeans_[0.05] = result_kmeans_005[0][0].tolist()\n",
        "result_kmeans_[0.075] = result_kmeans_0075[0][0].tolist()\n",
        "\n",
        "result_metis={}\n",
        "result_metis[1] = orig_curve[0]\n",
        "result_metis[0.1] = metis_dynamics_01_sin[0]\n",
        "result_metis[0.05] = metis_dynamics_005_sin[0]\n",
        "result_metis[0.075] = metis_dynamics_0075_sin[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peg5C7YhY9Ri"
      },
      "outputs": [],
      "source": [
        "n_steps=1000\n",
        "ICLR_compare_curves_2(result_new,n_steps)\n",
        "ICLR_compare_curves_2(result_competing,n_steps)\n",
        "ICLR_compare_curves_2(result_kmeans_,n_steps)\n",
        "ICLR_compare_curves_2(result_metis,n_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXL8H-UYE-7l"
      },
      "outputs": [],
      "source": [
        "ICLR_compare_rmse_2([result_competing, result_new, result_kmeans_,result_metis], ['Competing algorithm', 'Our algorithm', 'K-Means + AVG','Metis reduction'],0,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEePQnN-Zb25"
      },
      "outputs": [],
      "source": [
        "ICLR_compare_rmse_2([result_competing, result_new, result_kmeans_,result_metis], ['Competing algorithm', 'Our algorithm', 'K-Means + AVG','Metis reduction'],0,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWNt2fL4FKKc"
      },
      "outputs": [],
      "source": [
        "len(result_new[0.1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWxNweKRZmcY"
      },
      "outputs": [],
      "source": [
        "result_new = {}\n",
        "result_new[1] = orig_curve[1]\n",
        "result_new[0.1] = result_new_01[0][1].tolist()\n",
        "result_new[0.05] = result_new_005[0][1].tolist()\n",
        "result_new[0.075] = result_new_0075[0][1].tolist()\n",
        "result_competing = {}\n",
        "result_competing[1] = orig_curve[1]\n",
        "result_competing[0.1] = result_competing_01[0][1].tolist()\n",
        "result_competing[0.05] = result_competing_005[0][1].tolist()\n",
        "result_competing[0.075] = result_competing_0075[0][1].tolist()\n",
        "\n",
        "result_kmeans = {}\n",
        "result_kmeans[1] = orig_curve[1]\n",
        "result_kmeans[0.1] = result_kmeans_01[0][1].tolist()\n",
        "result_kmeans[0.05] = result_kmeans_005[0][1].tolist()\n",
        "result_kmeans[0.075] = result_kmeans_0075[0][1].tolist()\n",
        "\n",
        "result_metis={}\n",
        "result_metis[1] = orig_curve[1]\n",
        "result_metis[0.1] = metis_dynamics_01_sin[1]\n",
        "result_metis[0.05] = metis_dynamics_005_sin[1]\n",
        "result_metis[0.075] = metis_dynamics_0075_sin[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xz0Fjh70LaG"
      },
      "outputs": [],
      "source": [
        "n_steps=1000\n",
        "ICLR_compare_curves_2(result_new,n_steps)\n",
        "ICLR_compare_curves_2(result_competing,n_steps)\n",
        "ICLR_compare_curves_2(result_kmeans,n_steps)\n",
        "ICLR_compare_curves_2(result_metis,n_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSxRbcZ5FDqQ"
      },
      "outputs": [],
      "source": [
        "ICLR_compare_rmse_2([result_competing, result_new, result_kmeans,result_metis], ['Competing algorithm', 'Our algorithm', 'K-Means + AVG','Metis reduction'],0,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNeWyQaSzJHO"
      },
      "outputs": [],
      "source": [
        "ICLR_compare_rmse_2([result_competing, result_new, result_kmeans,result_metis], ['Competing algorithm', 'Our algorithm', 'K-Means + AVG','Metis reduction'],0,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Snc08GVp9PxN"
      },
      "outputs": [],
      "source": [
        "ICLR_compare_rmse_2([result_competing, result_new, result_kmeans,result_metis], ['Competing algorithm', 'Our algorithm', 'K-Means + AVG','Metis reduction'],1000,2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzNhRUmBImQq"
      },
      "source": [
        "##DMD sin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u28WyqQQIllE"
      },
      "outputs": [],
      "source": [
        "def get_dmd_estimation(X_train, rank):\n",
        "  est = dmd.DMD(svd_rank = rank, opt = True)\n",
        "  t0 = time.perf_counter()\n",
        "  #print('X_train.shape',X_train.shape)\n",
        "  X_train = X_train.numpy()\n",
        "  est.fit(X_train)\n",
        "  t1 = time.perf_counter()\n",
        "  est.dmd_time[\"tend\"] *= 2\n",
        "  return est.reconstructed_data, t1-t0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tctnf9mgNh97"
      },
      "outputs": [],
      "source": [
        "def ICLR_compare_curves_different_reduction_algs(result,labels,title='',steps_split=1000):\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "    for k, v in result.items():\n",
        "        ax.plot(v, label=labels[k], linewidth=4)\n",
        "\n",
        "    split_point = steps_split\n",
        "    ax.axvline(x=split_point, color='r', linestyle='--')\n",
        "    y_min, y_max = ax.get_ylim()\n",
        "    ax.text(split_point, y_max*0.9, 'Train/Test Split', rotation=0, verticalalignment='center', color='black',fontsize=24)\n",
        "\n",
        "    plt.legend(fontsize=20)\n",
        "    ax.set_ylabel('$p_{s},~Pa$', fontsize=20)\n",
        "    ax.set_xlabel('time steps', fontsize=20)\n",
        "    plt.xticks(fontsize = 20)\n",
        "    plt.yticks(fontsize = 20)\n",
        "    plt.title(title,fontsize = 20)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "result={}\n",
        "result[0]=x[task1['sinks'][0],:2000]\n",
        "result[1]=ground_truth_1[task1['sinks'][0],:2000]\n",
        "result[2]=storage_01[0.1][0][0].tolist()[:2000]\n",
        "ICLR_compare_curves_different_reduction_algs(result,[\"DMD\",\"ground truth\",\"our algorithm\"],\"sinusoidal permeability field\")\n",
        "\n",
        "result={}\n",
        "result[0]=x[task1['sinks'][1]]\n",
        "result[1]=ground_truth_1[task1['sinks'][1]]\n",
        "result[2]=storage_01[0.1][0][1].tolist()[:2000]\n",
        "ICLR_compare_curves_different_reduction_algs(result,[\"DMD\",\"ground truth\",\"our algorithm\"],\"sinusoidal permeability field\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXFdFW18ZuT1"
      },
      "source": [
        "##loop scenario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bhrplcTZxWX"
      },
      "outputs": [],
      "source": [
        "image = ''\n",
        "def image_to_graph(image, image_height=13):\n",
        "    image_width = int(len(image)/ image_height)\n",
        "    xs = np.linspace(0, 1, image_width)\n",
        "    ys = np.linspace(0, 1, image_height)\n",
        "    xs, ys = np.meshgrid(xs, ys)\n",
        "    xs, ys = xs.flatten(), ys.flatten()\n",
        "    ks = []\n",
        "    for i in image:\n",
        "        if i == '': ks.append(0.01)\n",
        "        else: ks.append(1)\n",
        "    return np.array([xs, ys, ks]).T\n",
        "def plot_diagram(points, ax, title=''):\n",
        "    vor = Voronoi(points[:, :2])\n",
        "    voronoi_plot_2d(vor, ax=ax, show_vertices=False)\n",
        "    ax.set_title(title)\n",
        "    bar1 = ax.scatter(points[:, 0], points[:, 1], c=points[:, 2], vmax=1, vmin=0, s=100)\n",
        "    for i, txt in enumerate(points):\n",
        "        ax.annotate(i, (points[i][0].item(), points[i][1].item()), fontsize=10, c='red')\n",
        "    plt.colorbar(bar1, ax=ax)\n",
        "eps = 1e-1\n",
        "boundary = torch.tensor([[-eps,-eps],[-eps, 1. + eps],[1. + eps, 1. + eps],[1. + eps, -eps]], requires_grad=False)\n",
        "\n",
        "sources = [0]\n",
        "sinks = [180]\n",
        "dt=0.0001\n",
        "n_steps=1000\n",
        "\n",
        "field = image_to_graph(image)\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "plot_diagram(field, ax)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scatter = plt.scatter(field[:, 0], field[:, 1], c=field[:, 2])\n",
        "#plt.title(\"Original permeability field (cos(20x) + sin(y))\")\n",
        "# s = list(task1['sources'].keys())\n",
        "# plt.scatter(x[s], y[s], marker='D', c='m')\n",
        "ii=0\n",
        "for j in sinks:\n",
        "  plt.scatter(field[j,0], field[j,1], s=100, c='g', marker='^')\n",
        "  plt.annotate('sensor'+str(ii+1), (field[j,0], field[j,1]),xytext=(-20.0, 0),textcoords='offset points', fontsize=16)\n",
        "  ii+=1\n",
        "\n",
        "ii=0\n",
        "for j in sources:\n",
        "  plt.scatter(field[j,0], field[j,1], s=100, c='r', marker='v')\n",
        "  plt.annotate('source'+str(ii+1), (field[j,0], field[j,1]), fontsize=16)\n",
        "  ii+=1\n",
        "plt.colorbar(scatter)"
      ],
      "metadata": {
        "id": "KbhrekM-P5t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaN8cD0YaBBn"
      },
      "outputs": [],
      "source": [
        "storage_loop_01, coarsener_loop_01 = try_different_degrees_of_reduction(field, boundary, dt, n_steps,\n",
        "                                                    reduction_degrees=[0.1], n_epochs=300,lr=0.01,sigmoid_weight = 0.1, phys_loss_weight = 100,\n",
        "                                                    aggregating_function=KMeans, method='gnn',\n",
        "                                                    sources={src: 100 for src in sources}, sinks=sinks,\n",
        "                                                    denseconv=False,gatconv=True, num_layers=3,temperature=0.4,n_steps_val=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zz3fV6jGaJl5"
      },
      "outputs": [],
      "source": [
        "storage_loop_0075, coarsener_loop_0075 = try_different_degrees_of_reduction(field, boundary, dt, n_steps,\n",
        "                                                    reduction_degrees=[0.075], n_epochs=300,lr=0.01,sigmoid_weight = 1.0, phys_loss_weight = 200,\n",
        "                                                    aggregating_function=KMeans, method='gnn',\n",
        "                                                    sources={src: 100 for src in sources}, sinks=sinks,\n",
        "                                                    denseconv=False,gatconv=True, num_layers=3,temperature=0.4,n_steps_val=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3umk-EoaU_C"
      },
      "outputs": [],
      "source": [
        "storage_loop_005, coarsener_loop_005 = try_different_degrees_of_reduction(field, boundary, dt, n_steps,\n",
        "                                                    reduction_degrees=[0.05], n_epochs=300,lr=0.02,sigmoid_weight = 1.0, phys_loss_weight = 300,\n",
        "                                                    aggregating_function=KMeans, method='gnn',\n",
        "                                                    sources={src: 100 for src in sources}, sinks=sinks,\n",
        "                                                    denseconv=False,gatconv=True, num_layers=3,temperature=0.4,n_steps_val=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrmYO95eOgID"
      },
      "outputs": [],
      "source": [
        "path='/content/gdrive/MyDrive/ML_ST/new_storage_loop_01.pt'\n",
        "torch.save(storage_loop_01, path)\n",
        "path='/content/gdrive/MyDrive/ML_ST/new_coarsener_loop_01.pt'\n",
        "torch.save(coarsener_loop_01, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yri1xqgFEl2i"
      },
      "outputs": [],
      "source": [
        "path='/content/gdrive/MyDrive/ML_ST/new_coarsener_loop_005.pt'\n",
        "torch.save(coarsener_loop_005, path)\n",
        "path='/content/gdrive/MyDrive/ML_ST/new_storage_loop_005.pt'\n",
        "torch.save(storage_loop_005, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2zgbetIHwi2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "path='/content/gdrive/MyDrive/ML_ST/new_storage_loop_0075.pt'\n",
        "torch.save(storage_loop_0075, path)\n",
        "\n",
        "path='/content/gdrive/MyDrive/ML_ST/new_storage_loop_005.pt'\n",
        "torch.save(storage_loop_005, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8C0oMaxIEZ2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "path='/content/gdrive/MyDrive/ML_ST/new_coarsener_loop_0075.pt'\n",
        "torch.save(coarsener_loop_0075, path)\n",
        "\n",
        "path='/content/gdrive/MyDrive/ML_ST/new_coarsener_loop_005.pt'\n",
        "torch.save(coarsener_loop_005, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-TAs8xoaXY8"
      },
      "outputs": [],
      "source": [
        "def metis_carsening_loop(path,n_clusters):\n",
        "    data = []\n",
        "    with open(path, \"r\") as file:\n",
        "        for line in file:\n",
        "            row = [float(x) for x in line.split()]\n",
        "            row[-1] = int(row[-1])\n",
        "            data.append(row)\n",
        "\n",
        "    data = np.array(data)\n",
        "    data[0][3]=n_clusters+1\n",
        "    data[180][3]=n_clusters+2\n",
        "    clusters = np.unique(data[:, 3])\n",
        "\n",
        "    centers_of_mass = {}\n",
        "\n",
        "    for cluster in clusters:\n",
        "        points = data[data[:, 3] == cluster][:, :3]\n",
        "        center=[0,0,0]\n",
        "        points[:,0]=points[:,0]*points[:,2]\n",
        "        points[:,1]=points[:,1]*points[:,2]\n",
        "        center[0]=points[:,0].sum()/points[:,2].sum()\n",
        "        center[1]=points[:,1].sum()/points[:,2].sum()\n",
        "        center[2]=points[:,2].mean()\n",
        "\n",
        "        centers_of_mass[cluster] = center\n",
        "\n",
        "    centers_xy=[]\n",
        "    for cluster_id, center in centers_of_mass.items():\n",
        "        centers_xy.append(center)\n",
        "\n",
        "\n",
        "    #centers_xy.append(field[180])\n",
        "    #centers_xy.append(field[0])\n",
        "\n",
        "    centers_xy=np.array(centers_xy)\n",
        "\n",
        "    x = centers_xy[:,0].reshape(-1, 1)\n",
        "    y = centers_xy[:,1].reshape(-1, 1)\n",
        "    k = centers_xy[:,2].reshape(-1, 1)\n",
        "    eps = 1e-1\n",
        "    task2 = {'field': np.concatenate([x, y, k], axis=1),\n",
        "            'sources': {src: 100. for src in [n_clusters]},\n",
        "            'sinks': [n_clusters+1],\n",
        "            'dt': 0.00001,\n",
        "            'boundary': torch.tensor([[-eps,-eps],\n",
        "                                      [-eps, l + eps],\n",
        "                                      [l + eps, l + eps],\n",
        "                                        [l + eps, -eps]], requires_grad=False),\n",
        "            'n_steps': 1000}\n",
        "\n",
        "    return task2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Lwr6Rq63YYz"
      },
      "outputs": [],
      "source": [
        "metis_loop_01_field=metis_carsening_loop('/content/output_points_loop_31.txt',31)\n",
        "metis_loop_0075_field=metis_carsening_loop('/content/output_points_loop_23.txt',23)\n",
        "metis_loop_005_field=metis_carsening_loop('/content/output_points_loop_15.txt',15)\n",
        "\n",
        "metis_dynamics_01, ani = dynamics(metis_loop_01_field['field'], sources=metis_loop_01_field['sources'],\n",
        "                             sinks=metis_loop_01_field['sinks'], boundary=boundary,\n",
        "                             dt=dt, save_animation=False, n_steps=2*n_steps)\n",
        "metis_dynamics_0075, ani = dynamics(metis_loop_0075_field['field'], sources=metis_loop_0075_field['sources'],\n",
        "                             sinks=metis_loop_0075_field['sinks'], boundary=boundary,\n",
        "                             dt=dt, save_animation=False, n_steps=2*n_steps)\n",
        "metis_dynamics_005, ani = dynamics(metis_loop_005_field['field'], sources=metis_loop_005_field['sources'],\n",
        "                             sinks=metis_loop_005_field['sinks'], boundary=boundary,\n",
        "                             dt=dt, save_animation=False, n_steps=2*n_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLl70KLl3k1Y"
      },
      "outputs": [],
      "source": [
        "ground_truth_2000_loop, ani = dynamics(field, sources={src: 100 for src in sources},\n",
        "                             sinks=sinks, boundary=boundary,\n",
        "                             dt=dt, save_animation=False, n_steps=3*n_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8oiJ1SAIwwO"
      },
      "outputs": [],
      "source": [
        "\n",
        "result_competing_method_loop=torch.load('/content/gdrive/MyDrive/icml_2025/result_competing_method_loop_lr_0.005.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR6muw0YBDTn"
      },
      "outputs": [],
      "source": [
        "storage_loop_0075 = torch.load('/content/gdrive/MyDrive/ML_ST/new_storage_loop_0075.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FACMU5sVA06n"
      },
      "outputs": [],
      "source": [
        "storage_loop_005 = torch.load('/content/gdrive/MyDrive/ML_ST/new_storage_loop_005.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfLm_ZiaA6vm"
      },
      "outputs": [],
      "source": [
        "storage_loop_01 = torch.load('/content/gdrive/MyDrive/ML_ST/new_storage_loop_01.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-BpR8L4Np9a"
      },
      "outputs": [],
      "source": [
        "# kmeans\n",
        "result_kmeans_loop = try_different_degrees_of_reduction(field, boundary, dt, n_steps,\n",
        "                                                                      reduction_degrees=[0.1, 0.075, 0.05], n_epochs=300,\n",
        "                                                                      aggregating_function=KMeans, method='kmeans',sources={src: 100 for src in sources}, sinks=sinks,run_wandb=False,clamp_x = [0, 1.0], clamp_y = [0, 1.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyqxREIE3qXB"
      },
      "outputs": [],
      "source": [
        "ground_truth_2000_loop, ani = dynamics(field, sources={src: 100 for src in sources},\n",
        "                             sinks=sinks, boundary=boundary,\n",
        "                             dt=dt, save_animation=False, n_steps=3*n_steps)\n",
        "\n",
        "\n",
        "orig_curve = ground_truth_2000_loop\n",
        "\n",
        "result_new_01 = storage_loop_01[0.1]\n",
        "result_new_005 = storage_loop_005[0.05]\n",
        "result_new_0075 = storage_loop_0075[0.075]\n",
        "\n",
        "result_competing_01 = result_competing_method_loop[0.1]\n",
        "result_competing_005 = result_competing_method_loop[0.05]\n",
        "result_competing_0075 =  result_competing_method_loop[0.075]\n",
        "\n",
        "result_kmeans_01 = result_kmeans_loop[0.1]\n",
        "result_kmeans_005 =result_kmeans_loop[0.05]\n",
        "result_kmeans_0075 = result_kmeans_loop[0.075]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qLUMoKs3tuO"
      },
      "outputs": [],
      "source": [
        "result_new = {}\n",
        "result_new[1] = np.array(orig_curve).flatten()\n",
        "result_new[0.1] = np.array(result_new_01[0]).flatten()\n",
        "\n",
        "result_new[0.05] = np.array(result_new_005[0]).flatten()\n",
        "result_new[0.075] =np.array( result_new_0075[0]).flatten()\n",
        "result_competing = {}\n",
        "result_competing[1] = np.array(orig_curve).flatten()\n",
        "result_competing[0.1] = np.array(result_competing_01[0]).flatten()\n",
        "result_competing[0.05] = np.array(result_competing_005[0]).flatten()\n",
        "result_competing[0.075] =np.array(result_competing_0075[0]).flatten()\n",
        "\n",
        "result_kmeans = {}\n",
        "result_kmeans[1] = np.array(orig_curve).flatten()\n",
        "result_kmeans[0.1] = np.array(result_kmeans_01[0]).flatten()\n",
        "result_kmeans[0.05] = np.array(result_kmeans_005[0]).flatten()\n",
        "result_kmeans[0.075] = np.array(result_kmeans_0075[0]).flatten()\n",
        "\n",
        "result_metis={}\n",
        "result_metis[1] = np.array(orig_curve).flatten()\n",
        "result_metis[0.1] = metis_dynamics_01[0]\n",
        "result_metis[0.05] = metis_dynamics_005[0]\n",
        "result_metis[0.075] = metis_dynamics_0075[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wE48YE2Nu8T"
      },
      "outputs": [],
      "source": [
        "n_steps=1000\n",
        "ICLR_compare_curves_2(result_new,n_steps)\n",
        "ICLR_compare_curves_2(result_competing,n_steps)\n",
        "ICLR_compare_curves_2(result_kmeans,n_steps)\n",
        "ICLR_compare_curves_2(result_metis,n_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfCfP98132zj"
      },
      "outputs": [],
      "source": [
        "ICLR_compare_rmse_2([result_competing, result_new, result_kmeans,result_metis], ['Competing algorithm', 'Our algorithm', 'K-Means + AVG','Metis reduction'],0,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK1Enmr133kU"
      },
      "outputs": [],
      "source": [
        "ICLR_compare_rmse_2([result_competing, result_new, result_kmeans,result_metis], ['Competing algorithm', 'Our algorithm', 'K-Means + AVG','Metis reduction'],1000,2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD1psOYCJscy"
      },
      "source": [
        "##DMD loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTInxBGSNud5"
      },
      "outputs": [],
      "source": [
        "def ICLR_compare_curves_different_reduction_algs(result,labels,title='',steps_split=1000):\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "    for k, v in result.items():\n",
        "        ax.plot(v, label=labels[k], linewidth=4)\n",
        "\n",
        "    split_point = steps_split\n",
        "    ax.axvline(x=split_point, color='r', linestyle='--')\n",
        "    y_min, y_max = ax.get_ylim()\n",
        "    ax.text(split_point, y_max*0.9, 'Train/Test Split', rotation=0, verticalalignment='center', color='black',fontsize=24)\n",
        "\n",
        "    plt.legend(fontsize=20)\n",
        "    ax.set_ylabel('$p_{s},~Pa$', fontsize=20)\n",
        "    ax.set_xlabel('time steps', fontsize=20)\n",
        "    plt.xticks(fontsize = 20)\n",
        "    plt.yticks(fontsize = 20)\n",
        "    plt.title(title,fontsize = 20)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "# result={}\n",
        "# result[0]=x[sinks[0][0],:500]\n",
        "# result[1]=ground_truth_1[task1['sinks'][0],:500]\n",
        "# result[2]=storage_01[0.1][0][0].tolist()[:500]\n",
        "# ICLR_compare_curves_different_reduction_algs(result,[\"DMD\",\"ground truth\",\"our algorithm\"],\"sinusoidal permeability field\")\n",
        "\n",
        "result={}\n",
        "result[0]=x[sinks[0]]\n",
        "result[1]=ground_truth_loop[sinks[0]]\n",
        "result[2]=storage_loop_01[0.1][0][0].tolist()[:2000]\n",
        "ICLR_compare_curves_different_reduction_algs(result,[\"DMD\",\"ground truth\",\"our algorithm\"],\"loop scenario\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_NQbMaHRxtr"
      },
      "source": [
        "## Checking generalizing ability on another grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDNk9U-SSKFs"
      },
      "outputs": [],
      "source": [
        "def inference_new(field,boundary,coarsener, dt=0.0001,n_steps=100,sources=None,sinks=None,num_layers=1, clusters=900,ground_truth=None):\n",
        "\n",
        "    ground_truth, _ = dynamics(field, sources={src: 100 for src in sources},\n",
        "                             sinks=sinks, boundary=boundary,\n",
        "                             dt=dt, save_animation=False, n_steps=n_steps)\n",
        "    FIELD = torch.tensor(field, requires_grad=False, dtype=torch.float)\n",
        "\n",
        "    mesh = triangulate(FIELD[:, :2].clone().detach())\n",
        "    #sources = list(sources.keys())\n",
        "\n",
        "    with torch.no_grad():\n",
        "      edge_index, areas, e, clipped_vertices_dict,_ = differentiable_voronoi(FIELD[:, :2], mesh, boundary)\n",
        "      h = calc_h(FIELD[:, :2], edge_index)\n",
        "      #print(perm.shape, h.shape, e.shape, edge_index.shape)\n",
        "\n",
        "      edge_weights = calc_physics_weights(FIELD[:, 2], e, h, edge_index)\n",
        "      edge_index, edge_weights = to_undirected(edge_index, edge_weights)\n",
        "      print('shape edge_weights ',edge_weights.shape)\n",
        "      print('shape edge_index ',edge_index.shape)\n",
        "      s, out,_ = coarsener(torch.unsqueeze(FIELD, 0), edge_index, edge_weights)\n",
        "\n",
        "    sources_reduced = list(range(len(out[0]) - len(sources),  len(out[0])))\n",
        "    sources_reduced = {src: 100 for src in sources_reduced}\n",
        "    sinks_reduced = list(range(len(out[0]) - len(sources) - len(sinks),  len(out[0]) - len(sources)))\n",
        "    print(sources_reduced, sinks_reduced)\n",
        "\n",
        "    print(out[0].shape)\n",
        "\n",
        "    reduced_dynamics, _ = dynamics(out[0], {src: 100 for src in sources_reduced},sinks_reduced, boundary, n_steps=n_steps, dt = dt)\n",
        "    print(reduced_dynamics.shape)\n",
        "\n",
        "    return reduced_dynamics,ground_truth,out[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKwSvBCxgbxB"
      },
      "outputs": [],
      "source": [
        "def inference_new_sinks(field,boundary,model_dict, dt=0.0001,n_steps=100,sources=None,sinks=None,num_layers=3, clusters=900,denseconv=False,gatconv=True):\n",
        "\n",
        "    ground_truth, _ = dynamics(field, sources={src: 100 for src in sources},\n",
        "                             sinks=sinks, boundary=boundary,\n",
        "                             dt=dt, save_animation=False, n_steps=n_steps)\n",
        "    FIELD = torch.tensor(field, requires_grad=False, dtype=torch.float)\n",
        "\n",
        "    mesh = triangulate(FIELD[:, :2].clone().detach())\n",
        "    #sources = list(sources.keys())\n",
        "    #coarsener=learnableCoarsening(channels=[3,8],k=clusters,sources=sources,sinks=sinks,temperature=0.4,denseconv=denseconv,gatconv=gatconv,num_layers=num_layers)\n",
        "    #coarsener.load_state_dict(model_dict)\n",
        "    with torch.no_grad():\n",
        "      edge_index, areas, e, clipped_vertices_dict,boundary_nodes = differentiable_voronoi(FIELD[:, :2], mesh, boundary)\n",
        "      h = calc_h(FIELD[:, :2], edge_index)\n",
        "      #print(perm.shape, h.shape, e.shape, edge_index.shape)\n",
        "\n",
        "      edge_weights = calc_physics_weights(FIELD[:, 2], e, h, edge_index)\n",
        "      edge_index, edge_weights = to_undirected(edge_index, edge_weights)\n",
        "      print('shape edge_weights ',edge_weights.shape)\n",
        "      print('shape edge_index ',edge_index.shape)\n",
        "\n",
        "      coarsener=learnableCoarsening(channels=[3,8],k=clusters,sources={src: 100 for src in sources},sinks=sinks,temperature=0.4,denseconv=denseconv,gatconv=gatconv,num_layers=num_layers,boundary_nodes=list(boundary_nodes))\n",
        "      coarsener.load_state_dict(model_dict)\n",
        "      s, out,_ = coarsener(torch.unsqueeze(FIELD, 0), edge_index, edge_weights)\n",
        "\n",
        "    sources_reduced = list(range(len(out[0]) - len(sources),  len(out[0])))\n",
        "    sources_reduced = {src: 100 for src in sources_reduced}\n",
        "    sinks_reduced = list(range(len(out[0]) - len(sources) - len(sinks),  len(out[0]) - len(sources)))\n",
        "    print(sources_reduced, sinks_reduced)\n",
        "\n",
        "    print(out[0].shape)\n",
        "\n",
        "    reduced_dynamics, _ = dynamics(out[0], {src: 100 for src in sources_reduced},sinks_reduced, boundary, n_steps=n_steps, dt = dt)\n",
        "    print(reduced_dynamics.shape)\n",
        "\n",
        "    mesh = triangulate(out[0][:, :2].clone().detach())\n",
        "    edge_index, areas, e, coarsened_clipped_vertices_dict,boundary_nodes = differentiable_voronoi(out[0][:, :2], mesh, boundary)\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(14, 10))\n",
        "    for vertex in coarsened_clipped_vertices_dict.keys():\n",
        "        line = coarsened_clipped_vertices_dict[vertex].clone().detach()\n",
        "        cmap = plt.get_cmap('RdYlBu')\n",
        "        p = matplotlib.patches.Polygon(line, facecolor = 'g', edgecolor='black', alpha=0.2, fill=False)\n",
        "        m = torch.mean(torch.unique(line, dim=0), dim=0)\n",
        "        ax1.add_patch(p)\n",
        "    scale = ax1.scatter(out[0].detach()[:, 0], out[0].detach()[:, 1], c=out[0].detach().numpy()[:, 2], s=400)\n",
        "    if sources is not None:\n",
        "        for j, src in enumerate(sources_reduced):\n",
        "            ax1.scatter(out[0].detach()[src, 0].item(), out[0].detach()[src, 1].item(), s=100, c='r', marker='v')\n",
        "            ax1.text(out[0].detach()[src, 0].item(), out[0].detach()[src, 1].item(), 'source ' + str(j+1), fontsize=30)\n",
        "\n",
        "    if sinks is not None:\n",
        "        for j, snk in enumerate(sinks_reduced):\n",
        "            ax1.scatter(out[0].detach()[snk, 0].item(), out[0].detach()[snk, 1].item(), s=100, c='g', marker='^')\n",
        "            ax1.text(out[0].detach()[snk, 0].item(), out[0].detach()[snk, 1].item(), 'sensor ' + str(j+1), fontsize=30)\n",
        "    cbar = plt.colorbar(scale)\n",
        "    cbar.ax.tick_params(labelsize=24)\n",
        "    cbar.ax.set_ylabel('$K(x, y)$', fontsize=40)\n",
        "\n",
        "    return reduced_dynamics,ground_truth,out[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Akr_A1QWde6"
      },
      "outputs": [],
      "source": [
        "def ICLR_compare_curves_inference(result,title=\"\"):\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "    for k, v in result.items():\n",
        "        ax.plot(v[0], label=str(k), linewidth=4)\n",
        "    plt.legend(fontsize=20)\n",
        "    ax.set_ylabel('$p_{s},~Pa$', fontsize=20)\n",
        "    ax.set_xlabel('time steps', fontsize=20)\n",
        "    plt.xticks(fontsize = 20)\n",
        "    plt.yticks(fontsize = 20)\n",
        "    plt.gca().set_title(title,fontsize = 20)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2sNEUFoOEIu"
      },
      "outputs": [],
      "source": [
        "coarsener_loop_01 = torch.load('/content/gdrive/MyDrive/ML_ST/new_coarsener_loop_01.pt',weights_only=False)\n",
        "#coarsener_loop_0075 = coar_loop[0.075]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQmivGhMOaD6"
      },
      "outputs": [],
      "source": [
        "coarsener_loop_0075"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MZA5vhpIslF"
      },
      "outputs": [],
      "source": [
        "reduced_dynamics_graphconv_179,ground_truth_graphconv_179,out_179=inference_new_sinks(field,boundary,coarsener_loop_0075[0.075].state_dict(),clusters = 23,dt=0.0001,n_steps=1000,sources=[0],sinks=[179])\n",
        "reduced_dynamics_graphconv_153,ground_truth_graphconv_153,out_153=inference_new_sinks(field,boundary,coarsener_loop_0075[0.075].state_dict(),clusters = 23,dt=0.0001,n_steps=1000,sources=[0],sinks=[128])\n",
        "reduced_dynamics_graphconv_101,ground_truth_graphconv_101,out_101=inference_new_sinks(field,boundary,coarsener_loop_0075[0.075].state_dict(),clusters = 23,dt=0.0001,n_steps=1000,sources=[0],sinks=[101])\n",
        "\n",
        "result_new_179 = {}\n",
        "result_new_153 = {}\n",
        "result_new_101 = {}\n",
        "result_new_179['new_sink_1,gt'] = ground_truth_graphconv_179\n",
        "result_new_153['new_sink_2,gt'] = ground_truth_graphconv_153\n",
        "result_new_101['new_sink_3,gt'] = ground_truth_graphconv_101\n",
        "\n",
        "result_new_179['reduced'] = reduced_dynamics_graphconv_179.tolist()\n",
        "result_new_153['reduced'] = reduced_dynamics_graphconv_153.tolist()\n",
        "result_new_101['reduced'] = reduced_dynamics_graphconv_101.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBm4bqlvgcXd"
      },
      "outputs": [],
      "source": [
        "field[179]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8hH_0VXW_VP"
      },
      "outputs": [],
      "source": [
        "field[177]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1Cecpp_XB6P"
      },
      "outputs": [],
      "source": [
        "field[101]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjFpuaJEPhps"
      },
      "outputs": [],
      "source": [
        "reduced_dynamics_graphconv_179,ground_truth_graphconv_179,out_179=inference_new_sinks(field,boundary,coarsener_loop_01[0.1].state_dict(),clusters = 31,dt=0.0001,n_steps=1000,sources=[0],sinks=[179])\n",
        "reduced_dynamics_graphconv_153,ground_truth_graphconv_153,out_153=inference_new_sinks(field,boundary,coarsener_loop_01[0.1].state_dict(),clusters = 31,dt=0.0001,n_steps=1000,sources=[0],sinks=[177])\n",
        "reduced_dynamics_graphconv_101,ground_truth_graphconv_101,out_101=inference_new_sinks(field,boundary,coarsener_loop_01[0.1].state_dict(),clusters = 31,dt=0.0001,n_steps=1000,sources=[0],sinks=[101])\n",
        "\n",
        "result_new_179 = {}\n",
        "result_new_153 = {}\n",
        "result_new_101 = {}\n",
        "result_new_179['new_sensor_1_gt'] = ground_truth_graphconv_179\n",
        "result_new_153['new_sensor_2_gt'] = ground_truth_graphconv_153\n",
        "result_new_101['new_sensor_3_gt'] = ground_truth_graphconv_101\n",
        "\n",
        "result_new_179['reduced_sensor_1'] = reduced_dynamics_graphconv_179.tolist()\n",
        "result_new_153['reduced_sensor_2'] = reduced_dynamics_graphconv_153.tolist()\n",
        "result_new_101['reduced_sensor_3'] =reduced_dynamics_graphconv_101.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYcKTnpoS2xJ"
      },
      "outputs": [],
      "source": [
        "ICLR_compare_curves_inference(result_new_179)\n",
        "ICLR_compare_curves_inference(result_new_153)\n",
        "ICLR_compare_curves_inference(result_new_101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZlFnjYycJfI"
      },
      "source": [
        "##Fine tuning decagon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhcWHC3Yhb4a"
      },
      "outputs": [],
      "source": [
        "seed=42\n",
        "set_seeds(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZZ7PiFaUFQt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from shapely.geometry import Point, Polygon\n",
        "import torch\n",
        "\n",
        "cx, cy = 2, 2\n",
        "r = 1\n",
        "num_points = 400\n",
        "\n",
        "angles = np.linspace(-2 * np.pi, 0, 10, endpoint=False)\n",
        "vertices = np.array([cx + r * np.cos(angles), cy + r * np.sin(angles)]).T\n",
        "\n",
        "def is_inside_octagon(x, y, vertices):\n",
        "    point = Point(x, y)\n",
        "    polygon = Polygon(vertices)\n",
        "    return polygon.contains(point)\n",
        "\n",
        "def generate_points_in_octagon(num_points, vertices):\n",
        "    points = []\n",
        "    while len(points) < num_points:\n",
        "        x, y = np.random.uniform(1, 3), np.random.uniform(1, 3)\n",
        "        if is_inside_octagon(x, y, vertices):\n",
        "            points.append((x, y))\n",
        "    return np.array(points)\n",
        "\n",
        "num_points = 400\n",
        "points = generate_points_in_octagon(num_points, vertices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91bORG7yhMAH"
      },
      "outputs": [],
      "source": [
        "def hold_points_in_octagon(x,y, vertices,N):\n",
        "    points = []\n",
        "    for i in range(N):\n",
        "        if is_inside_octagon(x[i][0].item(), y[i][0].item(),vertices):\n",
        "            points.append((x[i][0].item(), y[i][0].item()))\n",
        "    return np.array(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1sdsvCDhOAL"
      },
      "outputs": [],
      "source": [
        "eps = 1e-1\n",
        "vertices_boundary = np.array([cx + (r+eps) * np.cos(angles), cy + (r+eps) * np.sin(angles)]).T[::-1]\n",
        "vertices_boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKqTGATuhF_4"
      },
      "outputs": [],
      "source": [
        "\n",
        "l = 3.0\n",
        "nx = 25\n",
        "\n",
        "x, y = np.meshgrid(np.linspace(1, l, nx), np.linspace(1, l, nx))\n",
        "x = x.flatten().reshape(-1, 1)\n",
        "y = y.flatten().reshape(-1, 1)\n",
        "\n",
        "points=hold_points_in_octagon(x,y, vertices,nx*nx)\n",
        "print(points.shape)\n",
        "x,y=points[:, 0], points[:, 1]\n",
        "\n",
        "print(x.shape)\n",
        "x = x.flatten().reshape(-1, 1)\n",
        "y = y.flatten().reshape(-1, 1)\n",
        "\n",
        "k = np.cos(20 * x)+ np.sin(y)\n",
        "k = k + np.abs(np.min(k)) + 2.0\n",
        "eps = 1e-1\n",
        "task1 = {'field': np.concatenate([x, y, k], axis=1),\n",
        "        'sources': {src: 100. for src in [373]},\n",
        "        #'sinks': [21,32,395,406,224,206],\n",
        "        'sinks': [21],\n",
        "        'dt': 0.0001,\n",
        "        # 'boundary': torch.tensor([[-eps,-eps],\n",
        "        #                           [-eps, l + eps],\n",
        "        #                            [l + eps, l + eps],\n",
        "        #                             [l + eps, -eps]], requires_grad=False),\n",
        "        'boundary': torch.tensor(vertices_boundary.copy(), requires_grad=False),\n",
        "        'n_steps': 1000}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUm3sPGchIuE"
      },
      "outputs": [],
      "source": [
        "scatter = plt.scatter(task1['field'][:, 0], task1['field'][:, 1], c=task1['field'][:, 2])\n",
        "plt.title(\"Original permeability field (cos(20x) + sin(y))\")\n",
        "plt.colorbar(scatter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4BzB0gnhY1W"
      },
      "outputs": [],
      "source": [
        "ground_truth, ani = dynamics(task1['field'], sources=task1['sources'],\n",
        "                             sinks=task1['sinks'], boundary=task1['boundary'],\n",
        "                             dt=task1['dt'], save_animation=False, n_steps=task1['n_steps'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_yQPuBhilnn"
      },
      "outputs": [],
      "source": [
        "plt.plot(ground_truth[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYfPhmqZinkH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "\n",
        "scatter = plt.scatter(task1['field'][:, 0], task1['field'][:, 1], c=task1['field'][:, 2], norm=\"log\", cmap=cm.jet)\n",
        "#plt.clim(1e-3, 1e+3)\n",
        "plt.scatter(x[task1['sinks']], y[task1['sinks']], marker='D', c='k')\n",
        "s = list(task1['sources'].keys())\n",
        "plt.scatter(x[s], y[s], marker='D', c='m')\n",
        "ii=0\n",
        "for j in task1['sinks']:\n",
        "  plt.annotate(str(ii+1), (task1['field'][j,0], task1['field'][j,1]), fontsize=16)\n",
        "  ii+=1\n",
        "\n",
        "oct_x, oct_y = vertices[:,0],vertices[:,1]\n",
        "#plt.fill(oct_x, oct_y, 'lightblue', alpha=0.5)\n",
        "plt.plot(vertices[:, 0], vertices[:, 1], 'r-',color='black')\n",
        "plt.plot([oct_x[9],oct_x[0]], [oct_y[9],oct_y[0]], color='black')\n",
        "\n",
        "plt.title(\"Permeability\")\n",
        "plt.colorbar(scatter)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed=42\n",
        "set_seeds(seed)\n",
        "loss_curve, optimized_dmon, adjacency, optimized_coords = train(task1['field'],\n",
        "                                                                    sources=task1['sources'],\n",
        "                                                                    sinks=task1['sinks'],\n",
        "                                                                    ground_truth_dynamics=ground_truth,\n",
        "                                                                    boundary=task1['boundary'],\n",
        "                                                                    dt=task1['dt'], n_steps=task1['n_steps'], n_epochs=300,phys_loss_weight=500.0,lr=0.015,sigmoid_weight=1.0,clusters=40,\n",
        "                                                                    denseconv=False, num_layers=3,temperature=0.4,n_steps_val=0,warmup_steps=50,gatconv = True,seed=seed)"
      ],
      "metadata": {
        "id": "yCTRHlVC8IjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(optimized_dmon, \"/content/gdrive/MyDrive/ML_ST/optimized_dmon_full_clusters_40_decagon.pt\")\n",
        "torch.save(loss_curve, \"/content/gdrive/MyDrive/ML_ST/loss_curve_clusters_40_decagon.pt\")\n",
        "#torch.save(optimizer, \"/content/gdrive/MyDrive/ML_ST/optimizer_clusters_40_ten_angles.pt\")\n",
        "torch.save(optimized_coords, \"/content/gdrive/MyDrive/ML_ST/optimized_coords_clusters_40_decagon.pt\")"
      ],
      "metadata": {
        "id": "1MT9Ku3JBtCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRBKJapLxVyl"
      },
      "outputs": [],
      "source": [
        "seed=42\n",
        "set_seeds(seed)\n",
        "loss_curve, optimized_dmon, adjacency, optimized_coords = train(task1['field'],\n",
        "                                                                    sources=task1['sources'],\n",
        "                                                                    sinks=task1['sinks'],\n",
        "                                                                    ground_truth_dynamics=ground_truth,\n",
        "                                                                    boundary=task1['boundary'],\n",
        "                                                                    dt=task1['dt'], n_steps=task1['n_steps'], n_epochs=300,phys_loss_weight=500.0,lr=0.02,sigmoid_weight=1.0,clusters=40,\n",
        "                                                                    denseconv=False, num_layers=2,temperature=0.4,n_steps_val=0,warmup_steps=50,gatconv = True,seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdj6NaFWj_cw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eaz8RzKyk1pi"
      },
      "outputs": [],
      "source": [
        "torch.save(optimized_dmon, \"/content/gdrive/MyDrive/ML_ST/optimized_dmon_full_clusters_40_ten_angles.pt\")\n",
        "torch.save(loss_curve, \"/content/gdrive/MyDrive/ML_ST/loss_curve_clusters_40_ten_angles.pt\")\n",
        "#torch.save(optimizer, \"/content/gdrive/MyDrive/ML_ST/optimizer_clusters_40_ten_angles.pt\")\n",
        "torch.save(optimized_coords, \"/content/gdrive/MyDrive/ML_ST/optimized_coords_clusters_40_ten_angles.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqpW4lJIlxwj"
      },
      "outputs": [],
      "source": [
        "\n",
        "l = 3.0\n",
        "nx = 25\n",
        "starting_point=1.0\n",
        "\n",
        "\n",
        "x, y = np.meshgrid(np.linspace(starting_point, l, nx), np.linspace(starting_point, l, nx))\n",
        "x = x.flatten().reshape(-1, 1)\n",
        "y = y.flatten().reshape(-1, 1)\n",
        "\n",
        "#points=hold_points_in_octagon(x,y, vertices,nx*nx)\n",
        "# print(points.shape)\n",
        "# x,y=points[:, 0], points[:, 1]\n",
        "\n",
        "# print(x.shape)\n",
        "# x = x.flatten().reshape(-1, 1)\n",
        "# y = y.flatten().reshape(-1, 1)\n",
        "\n",
        "k = np.cos(20 * x)+ np.sin(y)\n",
        "k = k + np.abs(np.min(k)) + 2.0\n",
        "eps = 1e-1\n",
        "task2 = {'field': np.concatenate([x, y, k], axis=1),\n",
        "        'sources': {src: 100. for src in [18+25*19]},\n",
        "        #'sinks': [21,32,395,406,224,206],\n",
        "        'sinks': [6+25*3],\n",
        "        'dt': 0.0001,\n",
        "        'boundary': torch.tensor([[starting_point-eps,starting_point-eps],\n",
        "                                  [starting_point-eps, l + eps],\n",
        "                                   [l + eps, l + eps],\n",
        "                                    [l + eps, starting_point-eps]], requires_grad=False),\n",
        "        #'boundary': torch.tensor(vertices_boundary.copy(), requires_grad=False),\n",
        "        'n_steps': 1000}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XOwCWlYm12M"
      },
      "outputs": [],
      "source": [
        "FIELD = torch.tensor(task2['field'], requires_grad=False, dtype=torch.float)\n",
        "mesh = triangulate(FIELD[:, :2].clone().detach().numpy())\n",
        "#print(FIELD.shape, boundary.shape)\n",
        "edge_index, areas, e, clipped_vertices_dict,boundary_nodes = differentiable_voronoi(FIELD[:, :2], mesh, task2['boundary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3B3KnwfoYQq"
      },
      "outputs": [],
      "source": [
        "ground_truth, ani = dynamics(task2['field'], sources=task2['sources'],\n",
        "                             sinks=task2['sinks'], boundary=task2['boundary'],\n",
        "                             dt=task2['dt'], save_animation=False, n_steps=task2['n_steps'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20DHE91No7D-"
      },
      "outputs": [],
      "source": [
        "coarsener_decagon=torch.load(\"/content/gdrive/MyDrive/ML_ST/optimized_dmon_full_clusters_40_decagon.pt\",weights_only=False)\n",
        "optimized_dmon=coarsener_decagon"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed =42\n",
        "set_seeds(seed)\n",
        "loss_curve, optimized_dmon, adjacency, optimized_coords = train(task2['field'],\n",
        "                                                                    sources=task2['sources'],\n",
        "                                                                    sinks=task2['sinks'],\n",
        "                                                                    ground_truth_dynamics=ground_truth,\n",
        "                                                                    boundary=task2['boundary'],\n",
        "                                                                    dt=task2['dt'], n_steps=task1['n_steps'], n_epochs=20,phys_loss_weight=1000,lr=0.02,sigmoid_weight=1.0,\n",
        "                                                                    denseconv=False,num_layers=3,temperature=0.4,n_steps_val = 0,clusters=40,seed=seed,\n",
        "                                                                    coarsener=optimized_dmon,warmup_steps=0,\n",
        "                                                                    gatconv = True)"
      ],
      "metadata": {
        "id": "-VOOkOfnFu-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "set_seeds(seed)\n",
        "loss_curve, optimized_dmon, adjacency, optimized_coords = train(task2['field'],\n",
        "                                                                    sources=task2['sources'],\n",
        "                                                                    sinks=task2['sinks'],\n",
        "                                                                    ground_truth_dynamics=ground_truth,\n",
        "                                                                    boundary=task2['boundary'],\n",
        "                                                                    dt=task2['dt'], n_steps=task1['n_steps'], n_epochs=10,phys_loss_weight=500,lr=0.02,sigmoid_weight=1.0,\n",
        "                                                                    denseconv=False,num_layers=3,temperature=0.4,n_steps_val = 0,clusters=40,seed=seed,\n",
        "                                                                    coarsener=optimized_dmon,warmup_steps=0,\n",
        "                                                                    gatconv = True)"
      ],
      "metadata": {
        "id": "Jcwu1JDkCeCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed =42\n",
        "set_seeds(seed)\n",
        "loss_curve, optimized_dmon, adjacency, optimized_coords = train(task2['field'],\n",
        "                                                                    sources=task2['sources'],\n",
        "                                                                    sinks=task2['sinks'],\n",
        "                                                                    ground_truth_dynamics=ground_truth,\n",
        "                                                                    boundary=task2['boundary'],\n",
        "                                                                    dt=task2['dt'], n_steps=task1['n_steps'], n_epochs=20,phys_loss_weight=500,lr=0.01,sigmoid_weight=1.0,\n",
        "                                                                    denseconv=False,num_layers=3,temperature=0.4,n_steps_val = 0,clusters=40,seed=seed,\n",
        "                                                                    coarsener=optimized_dmon,warmup_steps=0,\n",
        "                                                                    gatconv = True)"
      ],
      "metadata": {
        "id": "a92MlcD5CJZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2Cc3qRu2-oU"
      },
      "outputs": [],
      "source": [
        "seed =42\n",
        "set_seeds(seed)\n",
        "loss_curve, optimized_dmon, adjacency, optimized_coords = train(task2['field'],\n",
        "                                                                    sources=task2['sources'],\n",
        "                                                                    sinks=task2['sinks'],\n",
        "                                                                    ground_truth_dynamics=ground_truth,\n",
        "                                                                    boundary=task2['boundary'],\n",
        "                                                                    dt=task2['dt'], n_steps=task1['n_steps'], n_epochs=20,phys_loss_weight=500,lr=0.02,sigmoid_weight=1.0,\n",
        "                                                                    denseconv=False,num_layers=3,temperature=0.4,n_steps_val = 0,clusters=40,seed=seed,\n",
        "                                                                    coarsener=optimized_dmon,warmup_steps=0,\n",
        "                                                                    gatconv = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18ou8AfkstHF"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "set_seeds(seed)\n",
        "loss_curve, optimized_dmon, adjacency, optimized_coords = train(task2['field'],\n",
        "                                                                    sources=task2['sources'],\n",
        "                                                                    sinks=task2['sinks'],\n",
        "                                                                    ground_truth_dynamics=ground_truth,\n",
        "                                                                    boundary=task2['boundary'],\n",
        "                                                                    dt=task2['dt'], n_steps=task1['n_steps'], n_epochs=20,phys_loss_weight=500,lr=0.02,sigmoid_weight=1.0,\n",
        "                                                                    denseconv=False,num_layers=2,temperature=0.4,n_steps_val = 0,clusters=40,seed=seed,\n",
        "                                                                    coarsener=optimized_dmon,warmup_steps=0,\n",
        "                                                                    gatconv = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "budYst63nnfL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZEeDA8u6xdH"
      },
      "source": [
        "##Fine tune other field"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "C5R3zEH4gYtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3Ngp3wB63MJ"
      },
      "outputs": [],
      "source": [
        "l = 1\n",
        "x, y = np.meshgrid(np.linspace(0, l, 20), np.linspace(0, l, 20))\n",
        "x = x.flatten().reshape(-1, 1)\n",
        "y = y.flatten().reshape(-1, 1)\n",
        "k = np.cos(20 * x)+ np.sin(5*y)\n",
        "k = k + np.abs(np.min(k)) + 2.0\n",
        "eps = 1e-1\n",
        "task2 = {'field': np.concatenate([x, y, k], axis=1),\n",
        "        'sources': {src: 100. for src in [210]},\n",
        "        'sinks': [42, 399-42 ],\n",
        "        'dt': 0.00001,\n",
        "        'boundary': torch.tensor([[-eps,-eps],\n",
        "                                  [-eps, l + eps],\n",
        "                                   [l + eps, l + eps],\n",
        "                                    [l + eps, -eps]], requires_grad=False),\n",
        "        'n_steps': 1000}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l48KR-z1_Zhe"
      },
      "outputs": [],
      "source": [
        "ground_truth, ani = dynamics(task2['field'], sources=task2['sources'],\n",
        "                             sinks=task2['sinks'], boundary=task2['boundary'],\n",
        "                             dt=task2['dt'], save_animation=False, n_steps=task2['n_steps'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYCuqAyC_qVn"
      },
      "outputs": [],
      "source": [
        "coarsener_sin=torch.load(\"/content/gdrive/MyDrive/ML_ST/new_coarsener_sin_01.pt\",weights_only=False)\n",
        "optimized_dmon=coarsener_sin[0.1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed =42\n",
        "loss_curve, optimized_dmon, adjacency, optimized_coords = train(task2['field'],\n",
        "                                                                    sources=task2['sources'],\n",
        "                                                                    sinks=task2['sinks'],\n",
        "                                                                    ground_truth_dynamics=ground_truth,\n",
        "                                                                    boundary=task2['boundary'],\n",
        "                                                                    dt=task2['dt'], n_steps=task2['n_steps'], n_epochs=20,phys_loss_weight=500,lr=0.01,sigmoid_weight=1.0,\n",
        "                                                                    denseconv=False,num_layers=3,temperature=0.4,n_steps_val = 0,clusters=40,\n",
        "                                                                    seed=seed,coarsener=optimized_dmon,warmup_steps=0)"
      ],
      "metadata": {
        "id": "6mNRDKQ8GnvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-rHQNa6gKnO"
      },
      "outputs": [],
      "source": [
        "seed =42\n",
        "loss_curve, optimized_dmon, adjacency, optimized_coords = train(task2['field'],\n",
        "                                                                    sources=task2['sources'],\n",
        "                                                                    sinks=task2['sinks'],\n",
        "                                                                    ground_truth_dynamics=ground_truth,\n",
        "                                                                    boundary=task2['boundary'],\n",
        "                                                                    dt=task2['dt'], n_steps=task2['n_steps'], n_epochs=20,phys_loss_weight=500,lr=0.01,sigmoid_weight=1.0,\n",
        "                                                                    denseconv=False,num_layers=3,temperature=0.4,n_steps_val = 0,clusters=40,\n",
        "                                                                    seed=seed,coarsener=optimized_dmon,warmup_steps=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCRSBtlQ_em0"
      },
      "outputs": [],
      "source": [
        "\n",
        "loss_curve, optimized_dmon, adjacency, optimized_coords = train(task2['field'],\n",
        "                                                                    sources=task2['sources'],\n",
        "                                                                    sinks=task2['sinks'],\n",
        "                                                                    ground_truth_dynamics=ground_truth,\n",
        "                                                                    boundary=task2['boundary'],\n",
        "                                                                    dt=task2['dt'], n_steps=task2['n_steps'], n_epochs=20,phys_loss_weight=500,lr=0.01,sigmoid_weight=1.0,\n",
        "                                                                    denseconv=False,num_layers=3,temperature=0.4,n_steps_val = 0,clusters=40,\n",
        "                                                                    seed=seed,coarsener=optimized_dmon,warmup_steps=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhppYY_6Cw7v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYo4AW_-AIEY"
      },
      "outputs": [],
      "source": [
        "reduced_dynamics_2,gt_2,coarsened_point_cloud_2 = inference_new(task2['field'], task2['boundary'],optimized_dmon, dt=task2['dt'],n_steps=10000,sources=list(task2['sources'].keys()),sinks=task2['sinks'], clusters = 40,ground_truth=ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehAQkPZfCQCG"
      },
      "outputs": [],
      "source": [
        "rmse_1 = torch.sqrt(torch.mean((reduced_dynamics_2[0]-gt_2[0])**2))\n",
        "rmse_2 = torch.sqrt(torch.mean((reduced_dynamics_2[1]-gt_2[1])**2))\n",
        "\n",
        "\n",
        "plt.plot(reduced_dynamics_2[0], label='reduced_sensor_1' + \" RMSE: {:1.4f}\".format(rmse_1.item()))\n",
        "plt.plot(reduced_dynamics_2[1], label='reduced_sensor_2' + \" RMSE: {:1.4f}\".format(rmse_2.item()))\n",
        "plt.plot(gt_2[0], label = 'gt1')\n",
        "plt.plot(gt_2[1], label = 'gt2')\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('p(t)')\n",
        "plt.title('Curves on cos(20*x) + sin(5*y)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM9qnQvbCyNL"
      },
      "outputs": [],
      "source": [
        "plt.plot(reduced_dynamics_2[0], label='reduced_sensor_1' + \" RMSE: {:1.4f}\".format(rmse_1.item()))\n",
        "#plt.plot(reduced_dynamics_2[1], label='reduced_sink_2' + \" RMSE: {:1.4f}\".format(rmse_2.item()))\n",
        "plt.plot(gt_2[0], label = 'gt1')\n",
        "#plt.plot(gt_2[1], label = 'gt2')\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('$p_{s}.~Pa$')\n",
        "plt.title('Curves on cos(20*x) + sin(5*y)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZsZBcqMFikh"
      },
      "outputs": [],
      "source": [
        "#plt.plot(reduced_dynamics_2[0], label='reduced_sink_1' + \" RMSE: {:1.4f}\".format(rmse_1.item()))\n",
        "plt.plot(reduced_dynamics_2[1], label='reduced_sensor_2' + \" RMSE: {:1.4f}\".format(rmse_2.item()))\n",
        "#plt.plot(gt_2[0], label = 'gt1')\n",
        "plt.plot(gt_2[1], label = 'gt2')\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('$p_{s}.~Pa$')\n",
        "plt.title('Curves on cos(20*x) + sin(5*y)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwrQ7ugPGeIU"
      },
      "outputs": [],
      "source": [
        "l = 1\n",
        "x, y = np.meshgrid(np.linspace(0, l, 20), np.linspace(0, l, 20))\n",
        "x = x.flatten().reshape(-1, 1)\n",
        "y = y.flatten().reshape(-1, 1)\n",
        "k = np.cos(15 * x)+ np.sin(5*y)\n",
        "k = k + np.abs(np.min(k)) + 2.0\n",
        "eps = 1e-1\n",
        "task2 = {'field': np.concatenate([x, y, k], axis=1),\n",
        "        'sources': {src: 100. for src in [210]},\n",
        "        'sinks': [42, 399-42 ],\n",
        "        'dt': 0.00001,\n",
        "        'boundary': torch.tensor([[-eps,-eps],\n",
        "                                  [-eps, l + eps],\n",
        "                                   [l + eps, l + eps],\n",
        "                                    [l + eps, -eps]], requires_grad=False),\n",
        "        'n_steps': 1000}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypgg7ITbFoLm"
      },
      "outputs": [],
      "source": [
        "ground_truth, ani = dynamics(task2['field'], sources=task2['sources'],\n",
        "                             sinks=task2['sinks'], boundary=task2['boundary'],\n",
        "                             dt=task2['dt'], save_animation=False, n_steps=task2['n_steps'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93B-0IwDGks7"
      },
      "outputs": [],
      "source": [
        "coarsener_sin=torch.load(\"/content/gdrive/MyDrive/ML_ST/new_coarsener_sin_01.pt\",weights_only=False)\n",
        "optimized_dmon=coarsener_sin[0.1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YzjuI5sGnPU"
      },
      "outputs": [],
      "source": [
        "seed=42\n",
        "loss_curve, optimized_dmon, adjacency, optimized_coords = train(task2['field'],\n",
        "                                                                    sources=task2['sources'],\n",
        "                                                                    sinks=task2['sinks'],\n",
        "                                                                    ground_truth_dynamics=ground_truth,\n",
        "                                                                    boundary=task2['boundary'],\n",
        "                                                                    dt=task2['dt'], n_steps=task2['n_steps'], n_epochs=20,phys_loss_weight=500,lr=0.01,sigmoid_weight=1.0,\n",
        "                                                                    denseconv=False,num_layers=3,temperature=0.4,n_steps_val = 0,clusters=40,\n",
        "                                                                    seed=seed,coarsener=optimized_dmon,warmup_steps=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6m92cWcGwL3"
      },
      "outputs": [],
      "source": [
        "reduced_dynamics_2,gt_2,coarsened_point_cloud_2 = inference_new(task2['field'], task2['boundary'],optimized_dmon, dt=task2['dt'],n_steps=10000,sources=list(task2['sources'].keys()),sinks=task2['sinks'], clusters = 40,ground_truth=ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_1 = torch.sqrt(torch.mean((reduced_dynamics_2[0]-gt_2[0])**2))\n",
        "rmse_2 = torch.sqrt(torch.mean((reduced_dynamics_2[1]-gt_2[1])**2))\n",
        "\n",
        "\n",
        "plt.plot(reduced_dynamics_2[0], label='reduced_sensor_1' + \" RMSE: {:1.4f}\".format(rmse_1.item()))\n",
        "plt.plot(reduced_dynamics_2[1], label='reduced_sensor_2' + \" RMSE: {:1.4f}\".format(rmse_2.item()))\n",
        "plt.plot(gt_2[0], label = 'gt1')\n",
        "plt.plot(gt_2[1], label = 'gt2')\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('$p_{s}.~Pa$')\n",
        "plt.title('Curves on cos(15*x) + sin(5*y)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "rkVNwo3yfjD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLpt2gfhGyj9"
      },
      "outputs": [],
      "source": [
        "rmse_1 = torch.sqrt(torch.mean((reduced_dynamics_2[0]-gt_2[0])**2))\n",
        "rmse_2 = torch.sqrt(torch.mean((reduced_dynamics_2[1]-gt_2[1])**2))\n",
        "\n",
        "plt.plot(reduced_dynamics_2[0], label='reduced_sensor_1' + \" RMSE: {:1.4f}\".format(rmse_1.item()))\n",
        "#plt.plot(reduced_dynamics_2[1], label='reduced_sink_2' + \" RMSE: {:1.4f}\".format(rmse_2.item()))\n",
        "plt.plot(gt_2[0], label = 'gt1')\n",
        "#plt.plot(gt_2[1], label = 'gt2')\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('$p_{s}.~Pa$')\n",
        "plt.title('Curves on cos(15*x) + sin(5*y)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKP0jBiVG0xg"
      },
      "outputs": [],
      "source": [
        "#plt.plot(reduced_dynamics_2[0], label='reduced_sink_1' + \" RMSE: {:1.4f}\".format(rmse_1.item()))\n",
        "plt.plot(reduced_dynamics_2[1], label='reduced_sensor_2' + \" RMSE: {:1.4f}\".format(rmse_2.item()))\n",
        "#plt.plot(gt_2[0], label = 'gt1')\n",
        "plt.plot(gt_2[1], label = 'gt2')\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('$p_{s}.~Pa$')\n",
        "plt.title('Curves on cos(15*x) + sin(5*y)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsgrF0LqJKws"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQRTC3pBJMm3"
      },
      "outputs": [],
      "source": [
        "l = 1\n",
        "x, y = np.meshgrid(np.linspace(0, l, 20), np.linspace(0, l, 20))\n",
        "x = x.flatten().reshape(-1, 1)\n",
        "y = y.flatten().reshape(-1, 1)\n",
        "k = np.cos(19 * x)+ np.sin(y)\n",
        "k = k + np.abs(np.min(k)) + 2.0\n",
        "eps = 1e-1\n",
        "task2 = {'field': np.concatenate([x, y, k], axis=1),\n",
        "        'sources': {src: 100. for src in [210]},\n",
        "        'sinks': [42, 399-42 ],\n",
        "        'dt': 0.00001,\n",
        "        'boundary': torch.tensor([[-eps,-eps],\n",
        "                                  [-eps, l + eps],\n",
        "                                   [l + eps, l + eps],\n",
        "                                    [l + eps, -eps]], requires_grad=False),\n",
        "        'n_steps': 1000}\n",
        "\n",
        "ground_truth, ani = dynamics(task2['field'], sources=task2['sources'],\n",
        "                             sinks=task2['sinks'], boundary=task2['boundary'],\n",
        "                             dt=task2['dt'], save_animation=False, n_steps=task2['n_steps'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sj6DFTBNWK5"
      },
      "outputs": [],
      "source": [
        "coarsener_sin=torch.load(\"/content/gdrive/MyDrive/ML_ST/new_coarsener_sin_01.pt\",weights_only=False)\n",
        "optimized_dmon=coarsener_sin[0.1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LKkSy1hNSHJ"
      },
      "outputs": [],
      "source": [
        "reduced_dynamics_2,gt_2,coarsened_point_cloud_2 = inference_new(task2['field'], task2['boundary'],optimized_dmon, dt=task2['dt'],n_steps=1000,sources=list(task2['sources'].keys()),sinks=task2['sinks'], clusters = 40,ground_truth=ground_truth)\n",
        "\n",
        "rmse_1 = torch.sqrt(torch.mean((reduced_dynamics_2[0]-gt_2[0])**2))\n",
        "rmse_2 = torch.sqrt(torch.mean((reduced_dynamics_2[1]-gt_2[1])**2))\n",
        "\n",
        "plt.plot(reduced_dynamics_2[0], label='reduced_sensor_1' + \" RMSE: {:1.4f}\".format(rmse_1.item()))\n",
        "plt.plot(reduced_dynamics_2[1], label='reduced_sensor_2' + \" RMSE: {:1.4f}\".format(rmse_2.item()))\n",
        "plt.plot(gt_2[0], label = 'gt1')\n",
        "plt.plot(gt_2[1], label = 'gt2')\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('$p_{s},~Pa$')\n",
        "plt.title('Curves on cos(19*x) + sin(y)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdOyeEGPLo5a"
      },
      "outputs": [],
      "source": [
        "l = 1\n",
        "x, y = np.meshgrid(np.linspace(0, l, 20), np.linspace(0, l, 20))\n",
        "x = x.flatten().reshape(-1, 1)\n",
        "y = y.flatten().reshape(-1, 1)\n",
        "k = np.cos(21 * x)+ np.sin(y)\n",
        "k = k + np.abs(np.min(k)) + 2.0\n",
        "eps = 1e-1\n",
        "task2 = {'field': np.concatenate([x, y, k], axis=1),\n",
        "        'sources': {src: 100. for src in [210]},\n",
        "        'sinks': [42, 399-42 ],\n",
        "        'dt': 0.00001,\n",
        "        'boundary': torch.tensor([[-eps,-eps],\n",
        "                                  [-eps, l + eps],\n",
        "                                   [l + eps, l + eps],\n",
        "                                    [l + eps, -eps]], requires_grad=False),\n",
        "        'n_steps': 1000}\n",
        "\n",
        "ground_truth, ani = dynamics(task2['field'], sources=task2['sources'],\n",
        "                             sinks=task2['sinks'], boundary=task2['boundary'],\n",
        "                             dt=task2['dt'], save_animation=False, n_steps=task2['n_steps'])\n",
        "coarsener_sin=torch.load(\"/content/gdrive/MyDrive/ML_ST/new_coarsener_sin_01.pt\",weights_only=False)\n",
        "optimized_dmon=coarsener_sin[0.1]\n",
        "\n",
        "reduced_dynamics_2,gt_2,coarsened_point_cloud_2 = inference_new(task2['field'], task2['boundary'],optimized_dmon, dt=task2['dt'],n_steps=1000,sources=list(task2['sources'].keys()),sinks=task2['sinks'], clusters = 40,ground_truth=ground_truth)\n",
        "\n",
        "rmse_1 = torch.sqrt(torch.mean((reduced_dynamics_2[0]-gt_2[0])**2))\n",
        "rmse_2 = torch.sqrt(torch.mean((reduced_dynamics_2[1]-gt_2[1])**2))\n",
        "\n",
        "plt.plot(reduced_dynamics_2[0], label='reduced_sensor_1' + \" RMSE: {:1.4f}\".format(rmse_1.item()))\n",
        "plt.plot(reduced_dynamics_2[1], label='reduced_sensor_2' + \" RMSE: {:1.4f}\".format(rmse_2.item()))\n",
        "plt.plot(gt_2[0], label = 'gt1')\n",
        "plt.plot(gt_2[1], label = 'gt2')\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('$p_{s},~Pa$')\n",
        "plt.title('Curves on cos(21*x) + sin(y)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b409hu19L-W8"
      },
      "outputs": [],
      "source": [
        "l = 1\n",
        "x, y = np.meshgrid(np.linspace(0, l, 20), np.linspace(0, l, 20))\n",
        "x = x.flatten().reshape(-1, 1)\n",
        "y = y.flatten().reshape(-1, 1)\n",
        "k = np.cos(20 * x)+ np.sin(1.5*y)\n",
        "k = k + np.abs(np.min(k)) + 2.0\n",
        "eps = 1e-1\n",
        "task2 = {'field': np.concatenate([x, y, k], axis=1),\n",
        "        'sources': {src: 100. for src in [210]},\n",
        "        'sinks': [42, 399-42 ],\n",
        "        'dt': 0.00001,\n",
        "        'boundary': torch.tensor([[-eps,-eps],\n",
        "                                  [-eps, l + eps],\n",
        "                                   [l + eps, l + eps],\n",
        "                                    [l + eps, -eps]], requires_grad=False),\n",
        "        'n_steps': 1000}\n",
        "\n",
        "ground_truth, ani = dynamics(task2['field'], sources=task2['sources'],\n",
        "                             sinks=task2['sinks'], boundary=task2['boundary'],\n",
        "                             dt=task2['dt'], save_animation=False, n_steps=task2['n_steps'])\n",
        "coarsener_sin=torch.load(\"/content/gdrive/MyDrive/ML_ST/new_coarsener_sin_01.pt\",weights_only=False)\n",
        "optimized_dmon=coarsener_sin[0.1]\n",
        "\n",
        "reduced_dynamics_2,gt_2,coarsened_point_cloud_2 = inference_new(task2['field'], task2['boundary'],optimized_dmon, dt=task2['dt'],n_steps=1000,sources=list(task2['sources'].keys()),sinks=task2['sinks'], clusters = 40,ground_truth=ground_truth)\n",
        "\n",
        "rmse_1 = torch.sqrt(torch.mean((reduced_dynamics_2[0]-gt_2[0])**2))\n",
        "rmse_2 = torch.sqrt(torch.mean((reduced_dynamics_2[1]-gt_2[1])**2))\n",
        "\n",
        "plt.plot(reduced_dynamics_2[0], label='reduced_sensor_1' + \" RMSE: {:1.4f}\".format(rmse_1.item()))\n",
        "plt.plot(reduced_dynamics_2[1], label='reduced_sensor_2' + \" RMSE: {:1.4f}\".format(rmse_2.item()))\n",
        "plt.plot(gt_2[0], label = 'gt1')\n",
        "plt.plot(gt_2[1], label = 'gt2')\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('$p_{s},~Pa$')\n",
        "plt.title('Curves on cos(20*x) + sin(1.5*y)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDDnIEo5L-GF"
      },
      "outputs": [],
      "source": [
        "l = 1\n",
        "x, y = np.meshgrid(np.linspace(0, l, 20), np.linspace(0, l, 20))\n",
        "x = x.flatten().reshape(-1, 1)\n",
        "y = y.flatten().reshape(-1, 1)\n",
        "k = np.cos(20 * x)+ np.sin(2.0*y)\n",
        "k = k + np.abs(np.min(k)) + 2.0\n",
        "eps = 1e-1\n",
        "task2 = {'field': np.concatenate([x, y, k], axis=1),\n",
        "        'sources': {src: 100. for src in [210]},\n",
        "        'sinks': [42, 399-42 ],\n",
        "        'dt': 0.00001,\n",
        "        'boundary': torch.tensor([[-eps,-eps],\n",
        "                                  [-eps, l + eps],\n",
        "                                   [l + eps, l + eps],\n",
        "                                    [l + eps, -eps]], requires_grad=False),\n",
        "        'n_steps': 1000}\n",
        "\n",
        "ground_truth, ani = dynamics(task2['field'], sources=task2['sources'],\n",
        "                             sinks=task2['sinks'], boundary=task2['boundary'],\n",
        "                             dt=task2['dt'], save_animation=False, n_steps=task2['n_steps'])\n",
        "coarsener_sin=torch.load(\"/content/gdrive/MyDrive/ML_ST/new_coarsener_sin_01.pt\",weights_only=False)\n",
        "optimized_dmon=coarsener_sin[0.1]\n",
        "\n",
        "reduced_dynamics_2,gt_2,coarsened_point_cloud_2 = inference_new(task2['field'], task2['boundary'],optimized_dmon, dt=task2['dt'],n_steps=1000,sources=list(task2['sources'].keys()),sinks=task2['sinks'], clusters = 40,ground_truth=ground_truth)\n",
        "\n",
        "rmse_1 = torch.sqrt(torch.mean((reduced_dynamics_2[0]-gt_2[0])**2))\n",
        "rmse_2 = torch.sqrt(torch.mean((reduced_dynamics_2[1]-gt_2[1])**2))\n",
        "\n",
        "plt.plot(reduced_dynamics_2[0], label='reduced_sensor_1' + \" RMSE: {:1.4f}\".format(rmse_1.item()))\n",
        "plt.plot(reduced_dynamics_2[1], label='reduced_sensor_2' + \" RMSE: {:1.4f}\".format(rmse_2.item()))\n",
        "plt.plot(gt_2[0], label = 'gt1')\n",
        "plt.plot(gt_2[1], label = 'gt2')\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('$p_{s},~Pa$')\n",
        "plt.title('Curves on cos(20*x) + sin(2.0*y)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agryzbgfLo1f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg1i08AtJShv"
      },
      "outputs": [],
      "source": [
        "ground_truth, ani = dynamics(task2['field'], sources=task2['sources'],\n",
        "                             sinks=task2['sinks'], boundary=task2['boundary'],\n",
        "                             dt=task2['dt'], save_animation=False, n_steps=task2['n_steps'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP5p7ipsJVO4"
      },
      "outputs": [],
      "source": [
        "coarsener_sin=torch.load(\"/content/gdrive/MyDrive/ML_ST/new_coarsener_sin_01.pt\",weights_only=False)\n",
        "optimized_dmon=coarsener_sin[0.1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC0wu_9YJXth"
      },
      "outputs": [],
      "source": [
        "reduced_dynamics_2,gt_2,coarsened_point_cloud_2 = inference_new(task2['field'], task2['boundary'],optimized_dmon, dt=task2['dt'],n_steps=1000,sources=list(task2['sources'].keys()),sinks=task2['sinks'], clusters = 40,ground_truth=ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoNu2q20JZ60"
      },
      "outputs": [],
      "source": [
        "plt.plot(reduced_dynamics_2[0], label='reduced_sink_1' + \" RMSE: {:1.4f}\".format(rmse_1.item()))\n",
        "plt.plot(reduced_dynamics_2[1], label='reduced_sink_2' + \" RMSE: {:1.4f}\".format(rmse_2.item()))\n",
        "plt.plot(gt_2[0], label = 'gt1')\n",
        "plt.plot(gt_2[1], label = 'gt2')\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('p(t)')\n",
        "plt.title('Curves on cos(15*x) + sin(5*y)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log uniform"
      ],
      "metadata": {
        "id": "-ADjZHZEK6xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import loguniform\n",
        "set_seeds(40)\n",
        "l = 1\n",
        "x, y = np.meshgrid(np.linspace(0, l, 30), np.linspace(0, l, 30))\n",
        "x = x.flatten().reshape(-1, 1)\n",
        "y = y.flatten().reshape(-1, 1)\n",
        "\n",
        "k = np.zeros(x.shape)\n",
        "q=0\n",
        "while q < len(x):\n",
        "    k[q] = loguniform.rvs(0.1, 10)\n",
        "    if k[q] > 0.1 and k[q]< 10:\n",
        "        q+=1\n",
        "\n",
        "eps = 1e-1\n",
        "task1 = {'field': np.concatenate([x, y, k], axis=1),\n",
        "        'sources': {src: 100. for src in [465]},\n",
        "        'sinks': [63],\n",
        "        'dt': 0.00005,\n",
        "        'boundary': torch.tensor([[-eps,-eps],\n",
        "                                  [-eps, l + eps],\n",
        "                                   [l + eps, l + eps],\n",
        "                                    [l + eps, -eps]], requires_grad=False),\n",
        "        'n_steps': 1000}"
      ],
      "metadata": {
        "id": "q2KiDjutK8t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scatter = plt.scatter(task1['field'][:, 0], task1['field'][:, 1], c=task1['field'][:, 2])\n",
        "plt.title(\"Original permeability field\")\n",
        "plt.colorbar(scatter)"
      ],
      "metadata": {
        "id": "yYrHPYcTjwMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth, ani = dynamics(task1['field'], sources=task1['sources'],\n",
        "                             sinks=task1['sinks'], boundary=task1['boundary'],\n",
        "                             dt=task1['dt'], save_animation=False, n_steps=task1['n_steps'])\n",
        "plt.plot(ground_truth[0])\n"
      ],
      "metadata": {
        "id": "tW-pJ9iSjqiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_competing_method = try_different_degrees_of_reduction(task1['field'], task1['boundary'], task1['dt'], task1['n_steps'], reduction_degrees=[0.1], n_epochs=300, aggregating_function=KMeans, method='points',\n",
        "                                                             sources = task1['sources'], sinks = task1['sinks'],clamp_x = [0, 1.0], clamp_y = [0, 1.0], lr = 0.001,save_ani=False)"
      ],
      "metadata": {
        "id": "IcAuEQW1juLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "storage, coarsener = try_different_degrees_of_reduction(task1['field'], task1['boundary'], task1['dt'], task1['n_steps'],\n",
        "                                                        reduction_degrees=[0.1], n_epochs=700, lr = 0.02, sigmoid_weight = 1.0, phys_loss_weight = 500,\n",
        "                                                        aggregating_function=KMeans, method='gnn', sources = task1['sources'], sinks = task1['sinks'], save_ani=False,\n",
        "                                                        denseconv=False, num_layers=3,temperature=0.4,n_steps_val=0,warmup_steps=50,gatconv = True)\n"
      ],
      "metadata": {
        "id": "rTVr1r8Tk5qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import loguniform\n",
        "set_seeds(41)\n",
        "l = 1\n",
        "x, y = np.meshgrid(np.linspace(0, l, 30), np.linspace(0, l, 30))\n",
        "x = x.flatten().reshape(-1, 1)\n",
        "y = y.flatten().reshape(-1, 1)\n",
        "\n",
        "k = np.zeros(x.shape)\n",
        "q=0\n",
        "while q < len(x):\n",
        "    k[q] = loguniform.rvs(0.01, 100)\n",
        "    if k[q] > 0.01 and k[q]< 100:\n",
        "        q+=1\n",
        "\n",
        "eps = 1e-1\n",
        "task1 = {'field': np.concatenate([x, y, k], axis=1),\n",
        "        'sources': {src: 100. for src in [465]},\n",
        "        'sinks': [63],\n",
        "        'dt': 0.000005,\n",
        "        'boundary': torch.tensor([[-eps,-eps],\n",
        "                                  [-eps, l + eps],\n",
        "                                   [l + eps, l + eps],\n",
        "                                    [l + eps, -eps]], requires_grad=False),\n",
        "        'n_steps': 5000}"
      ],
      "metadata": {
        "id": "PsRkJ_-jlScL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scatter = plt.scatter(task1['field'][:, 0], task1['field'][:, 1], c=task1['field'][:, 2])\n",
        "plt.title(\"Original permeability field\")\n",
        "plt.colorbar(scatter)"
      ],
      "metadata": {
        "id": "fCJ9tcO80NKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth, ani = dynamics(task1['field'], sources=task1['sources'],\n",
        "                             sinks=task1['sinks'], boundary=task1['boundary'],\n",
        "                             dt=task1['dt'], save_animation=False, n_steps=task1['n_steps'])\n",
        "plt.plot(ground_truth[0])\n"
      ],
      "metadata": {
        "id": "9iMJd4WY1rFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds(41)\n",
        "storage, coarsener = try_different_degrees_of_reduction(task1['field'], task1['boundary'], task1['dt'], task1['n_steps'],\n",
        "                                                        reduction_degrees=[0.1], n_epochs = 900, lr = 0.03, sigmoid_weight = 1.0, phys_loss_weight = 100,\n",
        "                                                        aggregating_function=KMeans, method='gnn', sources = task1['sources'], sinks = task1['sinks'], save_ani=False,\n",
        "                                                        denseconv=False, num_layers=3,temperature=0.4,n_steps_val=0,warmup_steps=200,gatconv = True)\n"
      ],
      "metadata": {
        "id": "ff3pgtybgtEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_competing_method = try_different_degrees_of_reduction(task1['field'], task1['boundary'], task1['dt'], task1['n_steps'], reduction_degrees=[0.1], n_epochs=300, aggregating_function=KMeans, method='points',\n",
        "                                                             sources = task1['sources'], sinks = task1['sinks'],clamp_x = [0, 1.0], clamp_y = [0, 1.0], lr = 0.001,save_ani=False)"
      ],
      "metadata": {
        "id": "13w_yMMp1zC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XVjv8Pka2jKL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}